{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from addict import Dict\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from stability.models.cnn import CBRNet, cnn_loss\n",
    "from stability.data import CellDataset\n",
    "from stability.train import train\n",
    "import yaml\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's construct data loaders. We need to refer to the `split.csv` file, which describes the paths to patches in the train, dev, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(os.environ[\"ROOT_DIR\"])\n",
    "data_dir = Path(os.environ[\"DATA_DIR\"])\n",
    "opts = Dict(yaml.safe_load(open(root_dir / \"conf/tnbc_cnn.yaml\", \"r\")))\n",
    "\n",
    "split_path = data_dir / opts.organization.xy\n",
    "split = pd.read_csv(split_path)\n",
    "sp = (\"train\", \"dev\", \"test\")\n",
    "img_paths = {k: split[split[\"split\"] == k].path.values for k in sp}\n",
    "ds = {k: CellDataset(img_paths[k], split_path, data_dir) for k in sp}\n",
    "loaders = {k: DataLoader(ds[k], batch_size=128) for k in sp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a directory for putting all our feature activations and training logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = data_dir / opts.organization.features_dir\n",
    "writer = SummaryWriter(features_dir / \"logs\")\n",
    "writer.add_text(\"conf\", json.dumps(opts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our model. Note that we have 7 input channels, since we are using 7 (grouped) cell types. Other than that, training is exactly like it was for the original cells data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30\n"
     ]
    }
   ],
   "source": [
    "model = CBRNet(p_in=7)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=opts.train.lr)\n",
    "\n",
    "out_paths = [\n",
    "    data_dir / opts.organization.features_dir, \n",
    "    data_dir / opts.organization.metadata,\n",
    "    data_dir / \"model_final.pt\"\n",
    "]\n",
    "train(model, optim, loaders, opts, out_paths, writer, cnn_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "li2",
   "language": "python",
   "name": "li2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
