{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're predicting the log tumor vs. immune cell ratio in these images. This is trivially computable from the cell counts from the different groups, but the key point is that this information is not necessarily obvious from the images (PD-1 might be a better response though...). All that's available immediately is the number of pixels belonging to each of the cell types.\n",
    "\n",
    "So, that can be our baseline. If we can predict better than just the pixel counts, then we have effectively trained a cell counter. We would expect to have learned features related to total cell count that aren't captured just in the pixel count (things like the cell size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sklearn.linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build this baseline, we first need to extract the proportion of per-image pixels belonging to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"/Users/ksankaran/Desktop/learned_inference/inference/vignettes/data/stability_data_tnbc/stability_data\")\n",
    "splits = pd.read_csv(data_dir / \"Xy.csv\")\n",
    "x = {\"train\": [], \"dev\": [], \"test\": []}\n",
    "y = {\"train\": [], \"dev\": [], \"test\": []}\n",
    "\n",
    "for p in splits.to_dict(orient=\"records\"):\n",
    "    patch = np.load(data_dir / p[\"path\"])\n",
    "    cell_means = np.mean(patch, axis=(0, 1))\n",
    "    x[p[\"split\"]].append(cell_means)\n",
    "\n",
    "for k in x.keys():\n",
    "    x[k] = np.stack(x[k])\n",
    "    y[k] = splits[\"y\"][splits[\"split\"] == k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit a simple ridge regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lm.Ridge()\n",
    "model.fit(x[\"train\"], y[\"train\"])\n",
    "y_hat = {\n",
    "    \"dev\": model.predict(x[\"dev\"]),\n",
    "    \"train\": model.predict(x[\"train\"]),\n",
    "    \"test\": model.predict(x[\"test\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the errors. It's also not hard to plot `y` vs. `y_hat` given the data that we've computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = {}\n",
    "for k in y_hat.keys():\n",
    "    err[k] = np.mean((y_hat[k] - y[k]) ** 2)\n",
    "\n",
    "json.dump(err, open(data_dir / \"baseline.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dev': 2.214406986885379,\n",
       " 'train': 1.0976247318043064,\n",
       " 'test': 1.9567960844047563}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "li",
   "language": "python",
   "name": "li"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
