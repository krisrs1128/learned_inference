{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a wrapper that helps us run a few types of models on a few splits of\n",
    "data. The outputs of this script are (1) saved features and (2) a trained model,\n",
    "selected to have the best dev set score.\n",
    "\n",
    "The main input parameters are the path to the `train_yaml` (relative to the root directory) and the bootstrap index to use. Since we may want to run this notebook as a python script (using `nbconvert`) we look up these arguments using environmental variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from addict import Dict\n",
    "from pathlib import Path\n",
    "from stability.data import initialize_loader\n",
    "from stability.models.vae import VAE, vae_loss\n",
    "from stability.models.cnn import CBRNet, cnn_loss\n",
    "import stability.models.random_features as rcf\n",
    "import stability.train as st\n",
    "import stability.train_rcf as srcf\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim\n",
    "import yaml\n",
    "\n",
    "#train_yaml = \"conf/tnbc_vae.yaml\"\n",
    "#bootstrap = 1\n",
    "#data_dir = Path(\"/Users/kris/Documents/stability_data/\")\n",
    "#root_dir = Path(\"/Users/kris/Desktop/conceptual/learned_inference\")\n",
    "train_yaml = os.environ[\"TRAIN_YAML\"]\n",
    "bootstrap = int(os.environ[\"BOOTSTRAP\"])\n",
    "data_dir = Path(os.environ[\"DATA_DIR\"])\n",
    "root_dir = Path(os.environ[\"ROOT_DIR\"])\n",
    "opts = Dict(yaml.safe_load(open(root_dir / train_yaml, \"r\")))\n",
    "print(opts.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create directories for saving all the features. We'll also read in all paths for training / development / testing. This is a bit more involved than the usual training process, since we'll want loaders specifically for looking at changes in feature activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = data_dir / opts.organization.features_dir\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "\n",
    "splits = pd.read_csv(data_dir / opts.organization.splits)\n",
    "resample_ix = pd.read_csv(data_dir / opts.bootstrap.path)\n",
    "\n",
    "paths = {\n",
    "    \"train\": splits.loc[splits.split == \"train\", \"path\"].values[resample_ix.loc[bootstrap]],\n",
    "    \"dev\": splits.loc[splits.split == \"dev\", \"path\"].values,\n",
    "    \"test\": splits.loc[splits.split == \"test\", \"path\"].values,\n",
    "    \"all\": splits[\"path\"].values\n",
    "}\n",
    "\n",
    "np.random.seed(0)\n",
    "save_ix = np.random.choice(len(splits), opts.train.save_subset, replace=False)\n",
    "loaders = {\n",
    "    \"train_fixed\": initialize_loader(paths[\"train\"], data_dir, opts),\n",
    "    \"train\": initialize_loader(paths[\"train\"], data_dir, opts, shuffle=True),\n",
    "    \"dev\": initialize_loader(paths[\"dev\"], data_dir, opts),\n",
    "    \"test\": initialize_loader(paths[\"test\"], data_dir, opts),\n",
    "    \"features\": initialize_loader(paths[\"all\"][save_ix], data_dir, opts)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the model and the loss functions. This is not super elegant, basically a long switch statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opts.train.model == \"cnn\":\n",
    "    model = CBRNet(nf=opts.train.nf, p_in=opts.train.p_in)\n",
    "    loss_fn = cnn_loss\n",
    "elif opts.train.model == \"vae\":\n",
    "    model = VAE(z_dim=opts.train.z_dim, p_in=opts.train.p_in)\n",
    "    loss_fn = vae_loss\n",
    "elif opts.train.model == \"rcf\":\n",
    "    patches = rcf.random_patches([data_dir / p for p in paths[\"train\"]], k=opts.train.n_patches)\n",
    "    model = rcf.WideNet(patches)\n",
    "else:\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's prepare a logger to save the training progress. We also save the indices of the samples for which we'll write activations -- it would be too much (and not really necessary) to write activations for all the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_path = data_dir / opts.organization.features_dir / \"subset.csv\"\n",
    "splits.iloc[save_ix, :].to_csv(subset_path)\n",
    "writer = SummaryWriter(features_dir / \"logs\")\n",
    "writer.add_text(\"conf\", json.dumps(opts))\n",
    "out_paths = [\n",
    "    data_dir / opts.organization.features_dir, # where features are saved\n",
    "    data_dir / opts.organization.metadata, # metadata for features (e.g., layer name)\n",
    "    data_dir / opts.organization.model # where model gets saved\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our model. Training for the random convolutional features model is just ridge regression -- there are no iterations necessary. For the CNN and VAE, all the real logic is hidden away in the `st.train` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opts.train.model == \"rcf\":\n",
    "    srcf.train_rcf(model, loaders, out_paths, alpha=opts.train.alpha, l1_ratio=opts.train.l1_ratio, normalize=True)\n",
    "else:\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=opts.train.lr)\n",
    "    st.train(model, optim, loaders, opts, out_paths, writer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "li",
   "language": "python",
   "name": "li"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
