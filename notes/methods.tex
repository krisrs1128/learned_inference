
\section{Algorithms}
\label{sec:algorithms}

Algorithms \ref{alg:features} through \ref{alg:selection} give our approach to
measuring feature stability. We first motivate the general setup, before
explaining specific choices used in our experiments.

The three algorithms do the following,
\begin{enumerate}
  \item Train a feature learning algorithm on $B$ perturbed versions of the
    dataset $\mathcal{D}$. This yields $B$ sets of learned features
    $\mathbf{Z}_{b}$.
  \item Use an alignment strategy $\mathcal{A}$ to represent the learned
    features in a shared coordinate system. These aligned features are called
    $\underaccent{\bar}{\mathbf{Z}}_{b}$.
  \item Using the aligned features, evaluate the importance of each feature
    dimension using a selection mechanism $\mathcal{S}$.
\end{enumerate}

The only subtlety is that the feature learning and selection steps are performed
on different subsets of $\mathcal{D}$, indexed by $I$ and $I^{C}$, respectively.
This is needed to maintain validity of inference -- if the same samples were
used for selection and learning, features would appear more important than they
are.

In our experiments, we use the bootstrap to perturb the data reserved for
feature learning. That is, if $I_{b}^{\ast}$ resamples $I$ with replacement,
then $\mathcal{D}\left[I^{\ast}_{b}\right]$ gives a draw from
$\mathcal{P}\left(\mathcal{D}\right)$. This lets us obtain $B$ sets of learned
features by optimizing
\begin{align*}
  \hat{\theta}_{b} := \arg\min_{\theta \in \Theta} \mathcal{L}\left(\mathcal{D}\left[I_{b}^{\ast}\right], T\left(\cdot; \theta\right)\right)
\end{align*}
and setting $\mathbf{Z}_{b} = T\left(\mathcal{D}, \hat{\theta}_{b}\right)$. This
step is summarized by Algorithm \ref{alg:features}.

\begin{algorithm}[H]
\SetAlgoLined
\KwResult{Sets of learned features $\left(\mathbf{Z}_{b}\right)_{b = 1}^{B}$ for
  $\mathcal{D}$. Indices $I\subset\left[n\right]$ used for feature learning.}
Inputs: Dataset $\mathcal{D}$ and perturbation process $\mathcal{P}$. Candidate
feature learners $\{T\left(\cdot; \theta\right)\}_{\theta \in \Theta}$ and
criterion $\mathcal{L}$.

1. Randomly split samples into disjoint subsets $I, I^{C} \subset \left[n\right]$.

2. Generate $B$ perturbed datasets,

\For{$b = 1, \dots, B$}{
	$\mathcal{D}_{b} \sim \mathcal{P}\left(\mathcal{D}\left[I\right]\right)$
}

3. Train $B$ feature learners,

\For{$b = 1, \dots, B$}{
  $\hat{\theta}_{b} = \arg\min_{\theta \in \Theta}\mathcal{L}\left(\mathcal{D}_{b}, T\left(\cdot, \theta\right)\right)$ \\
  $\mathbf{Z}_{b} = T\left(\mathcal{D}, \hat{\theta}_{b}\right)$
}
\caption{Feature Learning}
\label{alg:features}
\end{algorithm}

For the alignment strategy $\mathcal{A}$, we first reduce the dimensionality of
each $\mathbf{Z}_{b}$ to $K$ dimensions, call this $\tilde{\mathbf{Z}}_{b}$.
Then, we solve a generalized Procrustes problem, finding $\mathbf{R}_{b}$'s so
that the $\underaccent{\bar}{\mathbf{Z}}_{b} :=
\tilde{\mathbf{Z}}_{b}\mathbf{R}_{b}$ have low Frobenius distance to
$\mathbf{M}$. For the dimensionality reduction step, we use both PCA and SCA.
Given this $\mathcal{A}$, we can compute a feature subspace stability score
using Algorithm \ref{alg:fss}.

\begin{algorithm}[H]
  \SetAlgoLined
  \KwResult{Aligned features $\left(\underaccent{\bar}{\mathbf{Z}}_{1}, \dots,
    \underaccent{\bar}{\mathbf{Z}}_{B}\right)$. Subspace stability score $FSS_{\mathcal{A}}$.}
  Inputs: Learned features $\left(\mathbf{Z}_{1}, \dots, \mathbf{Z}_{B}\right)$.
  Alignment strategy $\mathcal{A}$.

  1. Align features,
  \begin{align*}
    M, \left(\underaccent{\bar}{\mathbf{Z}}_{1}, \dots, \underaccent{\bar}{\mathbf{Z}}_{B}\right) = \mathcal{A}\left(\mathbf{Z}_{1}, \dots, \mathbf{Z}_{B}\right)
  \end{align*}

  2. Compute feature subspace stability,
  \begin{align*}
    FSS_{\mathcal{A}}\left(\mathbf{Z}_{1}, \dots, \mathbf{Z}_{B}\right) = \frac{1}{B} \sum_{b = 1}^{B} \|\underaccent{\bar}{\mathbf{Z}}_{b} - \mathbf{M}\|^{2}_{2}.
  \end{align*}
  \caption{Feature Subspace Stability}
  \label{alg:fss}
\end{algorithm}

For the selection mechanism, we use stability selection. This means that our
selection mechanism $\mathcal{S}$ is parameterized by lasso regularization
strength $\lambda$ and selection stringency $\pi_{thr}$. In our experiments, we
display the full selection curves $\hat{\Pi}^{b}_{k}\left(\lambda\right)$ for
each set of aligned features. From these curves, we can identify important
features $S_{b}\left(\lambda, \pi_{thr}\right)$ for any choice of $\lambda$ or
$\pi_{thr}$. However, for clarity, we this suppress dependence on $\lambda$ and
$\pi_{thr}$ in Algorithm \ref{alg:selection}.

\begin{algorithm}[H]
  \SetAlgoLined
  \KwResult{Selection stability scores for all aligned dimensions $SS_{\mathcal{S}}^{1}, \dots, SS_{\mathcal{S}}^{K}$.}
  Inputs: Reserved indices $I^{C}$. Selection mechanism $\mathcal{S}$. Aligned
  features $\left(\underaccent{\bar}{\mathbf{Z}}_{1}, \dots,
  \underaccent{\bar}{\mathbf{Z}}_{B}\right)$, where each
  $\underaccent{\bar}{\mathbf{Z}}_{b} \in \reals^{n \times K}$.

  1. Define selection sets,

  \For{$b = 1, \dots, B$}{
    $S_{b} = \mathcal{S}\left(\underaccent{\bar}{\mathbf{Z}}_{b}\left[I^{C}\right]\right)$
  }

  2. Compute selection scores,

  \For{$k = 1, \dots, K$}{
    $SS_{\mathcal{S}}^{k} = \frac{1}{B}\sum_{b = 1}^{B} \indic{k \in S_{b}}$
  }
  \caption{Selection Stability}
  \label{alg:selection}
\end{algorithm}
