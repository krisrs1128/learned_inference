
\section{Reproducibility}

Instructions to reproduce our simulations and data analysis example are
available in a \href{https://github.com/krisrs1128/learned_inference}{README} on
the study's github page. Training split creation, the regression baseline, and
feature learning can be reproduced in the ipython notebooks

\begin{itemize}
\item \texttt{tnbc\_splits.ipynb}
\item \texttt{tnbc\_baseline.ipynb}
\item \texttt{model\_training.ipynb}
\end{itemize}

MIBI-ToF data preparation, generation of simulation data, feature stability
analysis, and visualization of results are done within the rmarkdown documents,

\begin{itemize}
\item \texttt{generate.Rmd}
\item \texttt{stability.Rmd}
\item \texttt{visualize\_features.Rmd}
\item \texttt{summary\_plots.Rmd}
\end{itemize}

To support code reusability between experiments, two helper packages were prepared,

\begin{itemize}
\item \href{https://github.com/krisrs1128/learned_inference/tree/master/stability}{\texttt{stability}}
\item \href{https://github.com/krisrs1128/learned_inference/tree/master/inference}{\texttt{inference}}
\end{itemize}

This packages can be installed by calling,

\begin{verbatim}
> git clone https://github.com/krisrs1128/learned_inference.git
> Rscript -e "devtools::install('learned_inference/inference')"
> pip3 install learned_inference/stability
\end{verbatim}
from the terminal.

We have prepared a \href{https://hub.docker.com/r/krisrs1128/li}{docker image}
with all necessary software installed. For example, to reproduce figure
\ref{fig:?}, you can enter the image and execute the relevant rmarkdown document
using the following commands,

\begin{verbatim}
shell> docker run -it krisrs1128/li:latest bash
docker shell> git clone https://github.com/krisrs1128/learned_inference.git
docker shell> source learned_inference/.env
docker shell> # download relevant data
docker shell> Rscript rmarkdown -e "rmarkdown::render('learned_inference/inference/vignettes/stability.Rmd')"
\end{verbatim}

Finally, we have released raw data and intermediate results from our analysis,

\begin{itemize}
\item
  \href{https://drive.google.com/file/d/1v_Ndux1Rmk2q1ul5Vv5srgI1JQ17Vx0n/view?usp=sharing}{sim\_data.tar.gz}:
  Our toy simulation dataset.
\item \href{https://drive.google.com/file/d/1KMG5yrty8UEPhrR0Y7hIZrtwWuP_y-cm/view?usp=sharing}{tnbc\_data.tar.gz}: The preprocessed MIBI-ToF data, with all
  patient's data split into 64 $\times$ 64 patches and with the associated
  splits and response value stored in a metadata file.
\item \href{https://drive.google.com/file/d/1QVmyqYQCe8C04rAyBQxXuZYuaiYleopx/view?usp=sharing}{simulation\_outputs.tar.gz}: All the models trained in our
  simulation experiments.
\item \href{https://drive.google.com/file/d/1DmzObBWCzVzDNZ1DxcUWgyIoZmC4H8gD/view?usp=sharing}{tnbc\_outputs.tar.gz}: All models trained in our illustration on
  the TNBC dataset.
\item \href{https://drive.google.com/file/d/1zJQOB2dSuy_1WYheJtUI8PZfu9PsyVGQ/view?usp=sharing}{simulation\_figure\_data.tar.gz}: The data written by
  `stability.Rmd` which was used to generate figures for our simulation.
\item \href{https://drive.google.com/file/d/1FaCrOysBlsNYgzul6iJiFcmgRFl2OaSE/view?usp=sharing}{tnbc\_figure\_data.tar.gz}: The data written by `stability.Rmd`
  which was used to generate figures for our data illustration.
\item \href{https://drive.google.com/file/d/1c-ZPs9RbzkY9FzUnCCVJc8syt5dpj-4Y/view?usp=sharing}{tnbc\_raw.tar.gz}:
  The original MIBI-ToF tiffs, before splitting into patches.
\end{itemize}

\section{Supplementary Figures}

\begin{figure}
\includegraphics[width=\textwidth]{correlation_heatmap_zoom}
\caption{A zoomed version of Figure \ref{fig:distributed_hm}, showing that the
  features cannot be directly mapped onto one another, from one run to the
  next.}
\label{fig:distributed_hm_subset}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{reconstructions}
\caption{Example reconstructions from the VAE model applied to the simulation
  data. Original image patches are shown on the left, corresponding
  reconstructions are given on the right. Though fine-grained details are missed
  by our version of the VAE model, most of the key global features of each patch
  seem accurately reflected.}
\label{fig:reconstructions}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{sim_embeddings-full-pca-1-2}
  \caption{The analogous figure to Figure \ref{fig:sim_embeddings-pca-1-2}, but
    keeping only the center of each bootstrap alignment, and showing the
    embeddings for all patches.}
  \label{fig:sim_embeddings-full-pca-1-2}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{sim_embeddings--sca-1-2}
  \caption{The analogous figure to Figure \ref{fig:sim_embeddings--pca-1-2}, but
    using SCA dimensionality reduction.}
  \label{fig:sim_embeddings--sca-1-2}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{sim_embeddings-full-sca-1-2}
  \caption{}
  \label{sim_embeddings-full-sca-1-2}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=1.4\textwidth]{sim_imagegrid-1-2}
  \caption{Patches overlaid on the learned features in Figure
    \ref{fig:embeddings50-test}, to guide interpretation of regions of the
    learned feature space. The figure is read in the same way as Figure
    \ref{fig:tnbc_imagegrid}.}
  \label{fig:sim_imagegrid-1-2}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=1.2\textwidth]{selection_paths-4}
  \includegraphics[width=1.2\textwidth]{selection_paths-1}
  \caption{Selection paths for all models when using 15\% of the data for
    feature learning. Most features, especially those from the CNN, have lower selection
    probability than the analogous features at other split sizes. Other key
    characteristics of the selection curves, like the effect of SVD vs. SCA
    reduction, and the stability of RCF features, remain the same.}
  \label{fig:selection_paths15}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=1.2\textwidth]{selection_paths-6}
  \includegraphics[width=1.2\textwidth]{selection_paths-3}
  \caption{Selection paths for all models when using 90\% of the data for
    feature learning. The top learned features across several methods are
    strongly related to the response; however, the width of the band across
    bootstraps has widened, relative to the analogous plot using 50\% of the
    data for feature learning.}
  \label{fig:selection_paths90}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{tnbc_selection_paths-1}
  \caption{Selection curves for the TNBC dataset application when using SCA for
    dimensionality reduction.}
  \label{fig:tnbc_selection_paths-1}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{tnbc_embeddings-3-6}
  \caption{Examples of features that are found to be unimportant in the TNBC
    dataset application, according to the selection curves in Figure
    \ref{fig:tnbc_selection_svd}. Note that the relationship between the
    displayed locations and the measured $y$ are much more ambiguous here,
    compared to those in Figure \ref{fig:tnbc_important_features}.}
  \label{fig:tnbc_embeddings-3-6}
\end{figure}

\begin{figure}
\centering
  \includegraphics[width=\textwidth]{tnbc_embeddings-full-both-1-2}
\end{figure}

\begin{figure}
\centering
  \includegraphics[width=\textwidth]{tnbc_embeddings-full-both-3-6}
\end{figure}

\begin{figure}
\centering
  \includegraphics[width=\textwidth]{tnbc_imagegrid-SCA-1-2}
\end{figure}
