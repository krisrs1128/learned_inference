\section{Introduction}

The traditional high-dimensional inference setting assumes that all features have been fixed in advance. When given a dataset $X \in \reals^{n \times p}$, there was little reason to ask where exactly the $p$ columns came from. However, across a variety of fields, the feature extraction process itself has become data-driven. Indeed, automatic feature learning can minimize the labor associated with manually extracting features. Consider the following examples,

\begin{itemize}
\item Spatial genomics: In addition to measuring the cell-level gene expression measurements of (tk give how many people), the study (tk give reference to stanford study) provided images describing the spatial relationships between cells. It has become possible to interrogate the ways in which collections of cells influence disease progress. (tk give study that actually uses VAE for this) A variety of features, like the amount of mixing between tumor and healthy cells, were first extracted from these images, before being used for survival analysis.
\item Computational Sustainability: The study () used satellite imagery to measure economic development across Nigeria. The results were in close agreement to traditional government surveys, suggesting that the method could be used as a less resource-intensive proxy. That work discusses the possibility that neighborhood characteristics and degree of infrastructure development were implicitly learned by the supervised model.
\end{itemize}

This shift from human-curated to machine-generated features poses a statistical challenge: How can robust scientific inference be accomplished when the targets of inference are themselves estimated from data? On the other hand, this shift also presents a statistical opportunity. As long as features were manually designed by investigators, it was hard to pinpoint exactly where they came from. The process was essentially a creative one; on another day, a different set of features might have been defined, and the statistician would have no way of knowing. The more systematic the feature generation process becomes, the easier it becomes to quantify the uncertainty that comes from it.

This question is not a new one — \cite{rao1955estimation} discussed tests for scores following a factor analysis, \cite{diaconis1983computer} described a bootstrap for principal components, \cite{elguero1988confidence} developed confidence regions for exploratory projection pursuit. This work revisits these ideas with a focus on the feature learning algorithms that have recently become common in the analysis of scientific imagery, especially those from deep learning.

It’s worth asking why the approaches cited above haven’t become already become standard practice the scientific workflow. Why isn’t the bootstrap already used to quantify the importance of cell type mixing in cancer progression, or the role of infrastructure development in economic growth? We suspect that the main reasons are,

\begin{itemize}
\item The learned features are effective for computational tasks, but not for human understanding. This is a result of the fact that deep learning representations are ``distributed'' \cite{mcclelland1986parallel}. Practically, this means that any pattern observed by the algorithm, will be encoded by a pattern of activations across a large number of more elementary features, not any specialized feature that recognizes just that pattern. A deep learning algorithm is able ot recognize a highway in a satellite image not because it has a single high-level feature for recognizing highways, but because it can merge evidence from across neurons (another word for an elementary learned feature) that activate when particular color, shape, and edge features are present. This approach turns out to be much more effective for tasks like classification and simulation, but poses a challenge for human inspection.
\item From one run to the next, the learned features don’t stay the same. This is unlike in principal components analysis, say, where the learned components are ordered in a natural way. If the deep learning features were more specialized, it might be possible to recognize the same feature across two runs, and then match them. However, the features are distributed, so it isn’t easy to say that any given neuron from the first run matches any other neuron(s) in the second. Coupled with the fact that there are regularly hundreds of neurons in any single layer of the model, it’s easy to see why an investigator might give up any hope of finding anything useful from bootstrapping their models.
\item It’s impractical to bootstrap methods that take hours to run, even if they could be done in parallel. Moreover, it’s unclear what information should be compared across bootstraps — the model parameters, the learned feature activations, or summary statistics about the features — and different choices have a major impact on the memory footprint of the proposal.
\item Some form of sample-splitting must take place, to ensure that the significance of features is not determined using the same data that was used to learn them. However, it’s unclear how the splitting should be carried out. How much data should be used for feature learning, and how much for inference? 
\end{itemize}

This work discusses these questions, providing definitions to guide future problem solving, providing simulations that explore natural solutions to the issues presented above, and illustrating these ideas through a feature analysis opf a spatial proteomics dataset. Our basic takeaways (in the order of the issues presented above) are,

\begin{itemize}
\item While the learned features are individually not interpretable, the associated feature subspaces often are. A feature learning algorithm may require a large number of elementary features in order to develop effective distributed representations, but the effective dimensionality of these representations is often small. These algorithms learn many, but highly correlated, features. The requirement that trainining be done with a large number of features is more an artifact of the optimization routines used to train these models than a fundamental property of the underlying scientific problem.
\item The underlying feature subspaces are often stable from one run to the next, and this can be quantified using a Procrustes analysis. There are several possible ways to identify a feature subspace — we use both the SVD and a sparse variant — and we find that those found using the sparse variant are generally more stable.
\item The stability of a sample within a learned feature space is often visible using only a handful of bootstrap resamples. Further, we find that in some problems, a fast approximation to full deep model training, called the random convolutional features model, can suffice for a feature stability analysis.
\item We find that, as long as the learned features lead to strong performance on a validation set, then it makes little difference how the samples are split. The fact that a relatively small fraction of the data can be reserved for inference is a consequence of the fact that the learned feature subspaces are effectively low dimensional; we do not need to perform inference on an overwhelming of orthogonal features.
\end{itemize}

Section \ref{sec:psetup} provides a description of our problem setting. Section \ref{sec:context} summarizes the key technical tools used in this study. We present a generic algorithm for measuring feature subspace stability in Section \ref{sec:algorithm}, and we study its properties through a simulation in Section \ref{sec:simulation}. We conclude with an application to a spatial proteomics dataset in \ref{sec:dataset}.

\section{Problem Setup}
\label{sec:psetup}

Our goal is to perform formal inference on features that were algorithmically derived. The features are derived from $n$ samples $x\_i \in \mathcal{X}$. For example, each $x_{i}$ might be a spatial proteomics or satellite image. They could also be more general data types, like the audio recording for the $i^{th}$ speaker or the graph for the $i^{th}$ molecule. Optionally, a vector $\mathbf{y}\in \reals^{n}$ of measured responses will be available.

\begin{definition}
A \textit{feature learner} is a parameterized mapping $T\left(\cdot;
\theta\right): \mathcal{X} \to \reals^{K}$ which takes data from the raw input
domain $\mathcal{X}$ and represents it using a vector in $\reals^{K}$.
\end{definition}

For example, in the transcriptomics and economics applications, we expect the encoder to transform a vector of raw pixel intensities into a vector of features reflecting cell type or infrastructural properties, respectively. The parameter $\theta$ is estimated from data. An unsupervised feature extractor estimates $\theta$ via $\hat{\theta}\left(x_{1}, \dots, x_{n}\right)$ while a supervised feature extractor uses $\hat{\theta}\left(x_{1}, \dots, x_{n}, y_{1}, \dots, y_{n}\right)$. To cover both cases, we will just write $\hat{\theta}$. For ease of notation, we will write $z_{i} = T\left(x_{i}; \hat{\theta}\right)$ to denote the learned features associated with the $i^{th}$ observation.

It is tempting to attribute an importance to each of the $k$ coordinates of $z$ with respect to a response $y$. For example, if $y$ measures food security level of a region, we would like to attribute this development to the presence of certain geographic features — e.g., the cultivation of a particular crop. The problem is to provide some sort of statistical guarantee for these attributions. An investigator should have confidence that, if the data were collected again, and if features were again extracted by some black box, then those that would be declared important would be closely related to those learned originally. The essential challenge is that the learned features are not the same from one run to the next; the $j^{th}$ feature from run 1 has nothing to do with the $j^{th}$ feature from run 2.

To address this issue, we distinguish between two notions of stability, which we call subspace and selection stability, respectively. The idea of subspace stability is that, even if there is no direct correspondence between learned features across runs, they may all reflect the same underlying latent features. Different runs of the feature learning algorithm return different bases for nearly the same subspace.
To make this more precise, suppose that the $b^{th}$ run of the feature learning algorithm produces,

\begin{align}
\mathbf{Z}_{b} &= \begin{pmatrix}
z^{b}_{1} \\
\vdots \\
z_{b}^{n}
\end{pmatrix} \in \reals^{n \times K}
\end{align}

and define an alignment function $\mathcal{A}$ which takes learned to aligned features,
\begin{align*}
\mathcal{A}: \mathbf{Z}_{1}, \dots, \mathbf{Z}_{B} \to \mathbf{M}, \left(\underaccent{\bar}{\mathbf{Z}}_{1}, \dots, \underaccent{\bar}{\mathbf{Z}}_{B}\right).
\end{align*}

We think of $\mathbf{M}$ as the average of all $B$ representations and $\underaccent{\bar}{\mathbf{Z}}_{b}$ as the version of the $b^{th}$ learned representation after they have been put into a common coordinate system. In this work, $\mathcal{A}$ is just a Procrustes analysis. With this notation, we can now define subspace stability.

\begin{definition}
The \textit{subspace stability} of $B$ learned representations $\mathbf{Z}_{1}, \dots, \mathbf{Z}_{B}$ with respect to an alignment function $\mathcal{A}$ is the distance from all aligned features and their average,
\begin{align*}
\frac{1}{B} \sum_{b = 1}^{B} \|\underaccent{\bar}{\mathbf{Z}}_{b} - \mathbf{M}\|^{2}_{2}.
\end{align*}
\end{definition}

By selection stability, we mean that a given aligned feature is repeatedly
discovered by a model selection procedure. At this point, the features can be
thought of as fixed; we are back in the more familiar statistical setting. That
is, let $\mathcal{S}$ be a selection function, which takes
$\underaccent{\bar}{\mathbf{Z}}_{b}$ and a response $y$ and returns a subset
$S_{b} \subseteq \{1, \dots, K\}$ of features that are important for
understanding variation in $y$.

\begin{definition}
The \textit{selection stability} of the $k^{th}$ aligned feature with respect to the selection function $\mathcal{S}$ is the fraction
\begin{align*}
\frac{1}{B}\sum_{b = 1}^{B} \indic{k \in \mathcal{S}\left(\underaccent{\bar}{\mathbf{Z}}_{b}, y\right)}.
\end{align*}
\end{definition}
