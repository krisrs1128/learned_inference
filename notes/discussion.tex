
\section{Discussion}

This study has examined the stability of algorithmically-derived, rather than
hand-crafted, features. A better understanding of stability in this modern
regime has consequences for how these methods can be used in real-world
applications, especially those intended for scientific workflows.

Our results raise several questions for further study. It is natural to ask
to what extent similar behaviors are exhibited across other data domains, model
types, or training regimes. For example, it would not be unusual to represent
the cell data in our case study using a marked graph linking neighboring cells.
Do the features learned by a graph autoencoder have similar stability
properties? In other domains, we may ask whether our methods can be adapted to
text or audio data.

( other perturbation functions )

Further, there are questions that may guide us towards better stability analysis
procedures.

While we have relied on the bootstrap, we have introduce notation to encompass
more general perturbations $\mathcal{P}$. For example, how do learned features
change when discarding nonrandomly chosen subsets of training data? Learning
features on different temporal or spatial subsets may reveal a drift in the
important features over time or space. Alternatively, we could imagine
perturbing the model training procedure, using different hyperparameters. It
would be of interest to trace out the dependence of the learned features on the
mechanics of learning procedure.

Similarly, using a Procrustes rotation for $\mathcal{A}$ and stability selection
for $\mathcal{S}$ provides a reasonable point of departure, but more
sophisticated approaches are possible. For example, dimensionality reduction
could be optimize to support alignment; this could be accomplished using
multiple canonical correlation analysis. Combinations of features could also be
approximately matched across feature learners, using a form of optimal
transport, identifying sets of features that all activate on similar input
samples. This has been applied in a federated learning context, but not to study
stability \citep{wang2020federated}.

It is also possible to propose alignments based on more refined measures of
correlation \citep{josse2016measuring, azadkia2019simple}. We have found that
using an even split between feature learning and inference and inference gives
reasonable results, but our results are admittedly coarse. Though we have
focused on stability selection, the procedure $\mathcal{S}$ could be any
selective inference procedure. Finally, our approaches to summarizing and
displaying the resulting representations may be improved through a more careful
application of interactive visualization principles.

(broader conversation on two cultures)

While the data sources and feature learning context we consider is novel, we
have been motivated by an old statistical idea that remains as true now as ever
-- quoting \citep{mosteller1977data},

\begin{quote}
One hallmark of the statistically conscious investigator is a firm belief that,
however the survey, experiment, or observational program actually turned out, it
could have turned out some somewhat differently.
\end{quote}

In order to accomplish this study, we have adapted tools from the
representational analysis, dimensionality reduction, and high-dimensional
inference communities. These tools give a window into the workings of modern
feature extraction techniques, helping us view commonplace algorithms in a new
way.

In a sense, our approach does more than summarize data â€” it generates new data.
In the same way that a standard error around a mean is a new piece of
information that supports more nuanced reasoning, the feature and selection
stability ideas we have developed help characterize the behavior of modern
feature learning algorithms.
