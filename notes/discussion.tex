
\section{Discussion}

This study has investigated the stability of machine-generated, rather than
hand-crafted, features. A better understanding of stability in this modern
regime has consequences for how these methods can be used in real-world
applications, especially those intended for scientific workflows.

Our results raise several questions for further study. It is natural to ask
to what extent similar behaviors are exhibited across other data domains, model
types, or training regimes. For example, it would not be unusual to represent
the cell data in our case study using a marked graph linking neighboring cells.
Do the features learned by a graph autoencoder have similar stability
properties? In other domains, we may ask whether our methods can be adapted to
text or audio data.

Further, there are questions that may guide us towards better instantiations of
Algorithms \ref{fig:learning} through \ref{fig:selection}. While we have relied
on the bootstrap, our notation encompasses more general perturbations
$\mathcal{P}$. For example, how might learned features change when discarding
nonrandomly chosen subsets of training data? Perhaps learning features based on
different temporal or spatial subsets could reveal a drift in the important
features over time or space. Alternatively, we could imagine perturbing the
model training procedure, using different hyperparameters. It would be of
interest to trace out the dependence of the learned features on the mechanics of
the feature learner.

Similarly, using a Procrustes rotation for $\mathcal{A}$ and stability selection
for $\mathcal{S}$ provides a reasonable point of departure, but more
sophisticated approaches are possible. For example, dimensionality reduction
could be optimized to support alignment; this could be accomplished using
multiple canonical correlation analysis. Combinations of features could also be
approximately matched across feature learners, using a form of optimal
transport, identifying sets of features that all activate on similar input
samples. This has been applied in a federated learning context, but not in the
study of stability \citep{wang2020federated}.

It is also possible to propose alignments based on more refined measures of
correlation \citep{josse2016measuring, azadkia2019simple}. We have found that
using an even split between feature learning and inference gives reasonable
results, but our results are admittedly coarse. Though we have concentrated on
stability selection, the procedure $\mathcal{S}$ could be any selective
inference procedure. Finally, our approaches to summarizing and displaying the
resulting representations may be improved through a more careful application of
interactive visualization principles.

More broadly, this study is situated in the body of work seeking to bridge the
rift between the two cultures \citep{breiman2001statistical,
  efron2020prediction}. Across the selective inference, conformal prediction,
and interpretability literatures, there is a growing understanding that the
statistics and machine learning communities could benefit by learning to speak
one another's languages \citep{angelopoulos2020uncertainty, ren2020knockoffs}.
Nonetheless, while the data sources and feature learning context we consider is
novel, our basic motivation is an old statistical idea that still rings true --
quoting from \citep{mosteller1977data},

\begin{quote}
One hallmark of the statistically conscious investigator is a firm belief that,
however the survey, experiment, or observational program actually turned out, it
could have turned out some somewhat differently.
\end{quote}

In order to accomplish this study, we have adapted tools from the
representational analysis, dimensionality reduction, and high-dimensional
inference communities. These tools give a window into the workings of modern
feature extraction techniques, helping us view commonplace algorithms in a new
way. In a sense, our intent is to do more than summarize data -- it is to
generates new data. In the same way that a standard error around a mean is a new
piece of information that supports more nuanced reasoning, the feature and
selection stability ideas we have developed can help characterize the behavior
of modern feature learning algorithms.
