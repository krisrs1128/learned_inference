
\section{Discussion}

This study has examined the stability of algorithmically-derived, rather than
hand-crafted, features. A better understanding of stability in this modern
regime has consequences for how these methods can be used in real-world
applications, especially those intended for scientific workflows.

Our results raise several questions for further study. It is natural to ask
to what extent similar behaviors are exhibited across other data domains, model
types, or training regimes. For example, it would not be unusual to represent
the cell data in our case study using a marked graph linking neighboring cells.
Do the features learned by a graph autoencoder have similar stability
properties? In other domains, we may ask whether our methods can be adapted to
text or audio data.

Further, there are questions that may guide us towards better stability analysis
procedures. Using a Procrustes rotation for $\mathcal{A}$ and stability
selection for $\mathcal{S}$ provides a reasonable point of departure, but more
sophisticated approaches are possible. For example, recent work has demonstrated
the potential to align learned features using optimal transport
\citep{wang2020federated }. It is also possible to propose alignments based on
more refined measures of correlation \citep{josse2016measuring,
  azadkia2019simple}. We have found that using an even split between
feature learning and inference and inference gives reasonable results, but our
results are not universal. Our approaches to summarizing and displaying the
resulting representations may be improved through a more careful application of
interactive visualization principles.

While the data sources and feature learning context we consider is novel, we
have been motivated by an old statistical idea that remains as true now as ever
-- quoting \citep{mosteller1977data},

\begin{quote}
One hallmark of the statistically conscious investigator is a firm belief that,
however the survey, experiment, or observational program actually turned out, it
could have turned out some somewhat differently.
\end{quote}

In order to accomplish this study, we have adapted tools from the
representational analysis, dimensionality reduction, and high-dimensional
inference communities. These tools give a window into the workings of modern
feature extraction techniques, helping us view commonplace algorithms in a new
way.

In a sense, our approach does more than summarize data â€” it generates new data.
In the same way that a standard error around a mean is a new piece of
information that supports more nuanced reasoning, the feature and selection
stability ideas we have developed help characterize the behavior of modern
feature learning algorithms.
