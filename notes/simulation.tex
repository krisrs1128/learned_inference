
\section{Simulation}

We use a simulation experiment to understand properties of the proposed
algorithms. Our key questions are,

\begin{enumerate}
\item How different are the features learned by supervised and unsupervised
  feature learning algorithms $T\left(\cdot; \theta\right)$?
\item Does the choice of dimensionality reduction used in $\mathcal{A}$ affect
  downstream inferences?
\item When we vary the relative sizes of $I$ and $I^{C}$, we expect a trade-off
  between feature learning and inferential quality. What is the nature of that
  trade-off?
\item Is it ever possible to substitute the RCF feature learner for either of
  the more time-consuming VAE or CNN models?
\end{enumerate}

To answer these questions, we evaluate our proposal using a full factorial with
three factors,

\begin{enumerate}
\item Relative sizes of $I$ and $I^{C}$: We use $\absarg{I} \in \left\{0.15n, 0.5n, 0.8n\right\}$.
\item Dimensionality reduction procedure: We use both PCA and sparse PCA.
\item Algorithm used: We train CNN, VAE and RCF models.
\end{enumerate}

\subsection{Simulated Data}

The key ingredient in this simulation is the construction of a dataset where all
``true'' features are directly controlled. To motivate the simulation, imagine
that we are examining a set of tumor pathology slides, with the hope of
recovering features that are predictive of survival time. Each pathology slide
gives a view into a cellular ecosystem — there are several types of cells, with
different sizes, density, and affinities to one another.

In the simulation, we first generate these underlying properties of each slide.
Survival is defined as a linear function of these underlying properties. Then,
images are generated that reflect these properties. The essential characteristic
of this simulation is that the analyst only has access to the images and
survival times, not the true predictors of the linear model. A good feature
extractor $T\left(x; \hat{\theta}\left(\mathcal{D}\right)\right)$ should recover
these important cell ecosystem properties from the images alone. Example images
for varying values of $y$ are given in Figure \ref{fig:matern_example}.

We now give details. Our images are simulated as two-dimensional marked Log Cox
Matern Process (LCMP) \cite{diggle2013}. Recall that a Matern process is a
Gaussian process with covariance function

\begin{align*}
C_{\nu, \alpha}(\|x - y\|)=\sigma^{2} \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\sqrt{2 \nu} \frac{\|x - y\|}{\alpha}\right)^{\nu} K_{\nu}\left(\sqrt{2 \nu} \frac{\|x - y\|}{\alpha}\right).
\end{align*}

$\alpha$ acts like a bandwidth parameter and $\nu$ controls the roughness of the
simulated process. A LCMP process with $K$ classes is constructed as follows.
First, a nonnegative process $\Lambda\left(x\right)$ is simulated along the
image grid, $\Lambda\left(x\right) \exp{\mathcal{N}\left(0,
  \mathbf{C}_{\nu_{\Lambda}, \alpha_{\Lambda}}\right)}$, where
$\mathbf{C}_{\nu_{\Lambda}, \alpha_{\Lambda}}$ is the covariance matrix induced
by $C_{\nu_{\Lambda}, \alpha_{\Lambda}}$. This represents a baseline intensity
that determines the location of cells, regardless of cell type. Then, $K$
further processes are simulated, $B_{k}\left(x\right) \exp{b_{k} +
  \mathcal{N}\left(0, \mathbf{C}_{\nu_{B}, \alpha_{B}}\right)} $. These
processes will reflect the relative frequencies of the $k$ classes at any
location $x$; the intercept $b_k$ makes a class either more or less frequent
across all positions $x$.

Given these intensity functions, we can simulate $N$ cell locations by drawing
from an inhomogeneous Poisson process with intensity $\Lambda\left(x\right)$.
For a cell at location $x$, we assign it cell type $k$ with probability
$\frac{B_{k}^{\tau}\left(x\right)}{\sum_{k^\prime = 1}^{K}
  B^{\tau}_{k^\prime}\left(x\right)}$. Here, we have introduced a temperature
parameter $\tau$ which controls the degree of mixedness between cell types at a
given location.

To complete the procedure for simulating images, we add two last source of
variation — cell size. The cells from class $k$ are drawn with a random radius
drawn from $\text{Gamma}\left(5, \lambda_{k}\right)$. A summary of all
parameters used to generate each image is given in Table \ref{tab:sim_params}.
These parameters are the ``true'' underlying features associated with the
observed images; they give the most concise description of the variation
observed across the images.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{generation_mechanism}
  \caption{Example simulated images, for low (top), average (middle), and high
    (bottom) values of $y$. For each sample, three relative intensity functions
    are generated, shown as a greyscale heatmap (left three displays for each
    group). Samples drawn from each process are overlaid as circles. The final
    images (display at the right for each group) overlay the points from the
    three processes, discarding the original generating intensity function.
    Evidently, small $y$ values are associated with smoother, less structured
    intensity functions.}
  \label{fig:matern_example}
\end{figure}

\begin{table}[]
\begin{tabular}{|p{0.1\linewidth}|p{0.4\linewidth}|p{0.16\linewidth}|p{0.15\linewidth}|}
\hline
\textbf{Feature}              & \textbf{Description}                                 & \textbf{Influence}          & \textbf{Range}             \\
\hline
$N_i$                & The total number of cells.                                    & 0.5                         & $\left[50, 1000\right]$    \\
\hline
$\nu_{i,\Lambda}$    & The roughness of the underlying intensity process.            & -0.5                        & $\left[0, 8\right]$        \\
\hline
$\alpha_{i,\Lambda}$ & The roughness of the underlying intensity process.            & -0.5                        & $\left[0, 8\right]$        \\
\hline
$b_{ik}$             & The intercept controlling the overall frequency of class $k$. & 1 for $k = 1$,\ -1 otherwise & $\left[-0.15, 0.15\right]$ \\
\hline
$\nu_{iB}$           & The roughness of the relative intensity processes.            & -0.5                        & $\left[0, 3\right]$        \\
\hline
$\alpha_{iB}$        & The bandwidth of relative intensity processes.                & -0.5                        & $\left[0, 3\right]$        \\
\hline
$\tau_{i}$           & The temperature used in cell type assignment.                 & 0.5                         & $\left[0, 3\right]$        \\
\hline
$\lambda_{ik}$       & The shape parameter controlling the sizes of each cell type.  & 1 for $k = 1$,\ 0 otherwise  & $\left[100, 500\right]$   \\
\hline
\end{tabular}
\caption{Our simulation mechanism is governed by the above parameters.
  Parameters $N_i$ and $\lambda_{ik}$ control the number and sizes of imaginary
  cells. $\nu_{i, \Lambda}$, $\alpha_{i, \Lambda}$, $b_{ik}$, $\nu_{iB}$,
  $\alpha_{iB}$, and $\tau_{i}$ control the overall and relative intensities of
  the Matern process from which the cell positions are drawn. Example draws are
  displayed in Figure \ref{fig:matern_example}}
\label{tab:sim_params}
\end{table}


These features are the underlying properties used to generate survival time.
Specifically, we generate a survival time using $y_i = \sum_{k}
\text{Influence}\text{Feature}_{ik}$ for the values Influence given in Table
\ref{tab:sim_params}. Note that there is no additional noise: if the
$\text{Feature}_{ik}$'s were known for each sample, then the $y$'s could be
predicted perfectly. Therefore, the simulation gauges the capacity of the
feature learners to recover these known underlying features. These features are
themselves all drawn from uniform distributions between the ranges given in the
table.

\subsection{Distributed features}

Figure \ref{fig:distributed_hm} shows the activations of learned features across
2000 samples for two bootstrap resamples from each of the feature learning
algorithms when using 90\% of the training data for feature learning. In the
three cases, the learned features correspond to

\begin{itemize}
\item CNN: Activations from the last layer of neurons, used directly as input
  for the regression. There are a total of 512 nonnegative
  features\footnote{They are nonnegative because they follow an ReLU layer.}.
\item VAE: Spatially-pooled activations from the middle, encoding layer of the
  variational autoencoder. There are a total of 64 real-valued features.
\item RCF: The spatially-pooled activations corresponding to each of 1048 random
  convolutional features.
\end{itemize}

Our first observation is that there is no simple correspondence between learned
and source features (i.e., parameters of the underlying simulation). For
example, it is not the case that one set of features represents the number of
cells $N$, and a different set maps to the roughness $\alpha, \nu$. Rather,
there appear to be clusters of learned features, and each cluster corresponds to
a pattern of correlation across multiple source features. We also find large
subsets of features, across all models, that are only weakly correlated with any
source feature.

Next, note that certain source features are ``easier'' to represent than others,
in the sense that more of the learned features are strongly correlated with
them. Many features are correlated with $N_{i}$, the total number of cells in
the image, and $\lambda_{i1}$, the size of the cells from Process 1. Depending
on the model the process roughness $\alpha_{ir}$, $\nu_{ir}$ and prevalence
$\beta_{ik}$ parameters are either only weakly or not at all correlated with the
learned features. Interestingly, the convolutional network learns to represent
$\beta_{1}$ well, but not $\beta_{2}$ or $\beta_{3}$ -- this is consistent with
the fact that only $\beta_{1}$ influences the response $y_{i}$.

Finally, we observe that different models learn qualitatively different sets of
features. For example, the CNN features tend to be more clustered, with strong
correlation across several source features. In contrast, the RCF features show
more gradual shifts in correlation strength. They also show relatively little
variation in correlation strength across features other than $\lambda_{i1}$ and
$N_{i}$.

Note that the features do not naturally map onto one another from one run to the
next. This is not obvious from Figure \ref{fig:distributed_hm}, but a zoomed in
version in Figure \ref{fig:distributed_hm_subset} showing only the first 15
features per run makes this clear.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{correlation_heatmap}
  \caption{Each feature learning algorithm (rows) learns a distributed
    representation of the true underlying features in the simulation. The two
    columns give results from two bootstrap resamples. Within each heatmap, rows
    correspond to the parameters in Table \ref{tab:sim_params}. Columns are
    activations of learned features; they have been reordered by a hierarchical
    clustering. The color of a cell gives the correlation between the associated
    true and learned features. Green and burgundy encode positive and negative
    correlation, respectively. Evidently, all learned features are associated
    with multiple true features at once.}
  \label{fig:distributed_hm}
\end{figure}

\subsection{Feature Learning vs. Inference}

It is natural to ask how the overall correlation between source and learned
features vary as a function of the algorithms and training sample sizes used. To
this end, Figure \ref{fig:cca_sim} displays the top canonical correlations
between learned features $\underaccent{\bar}{\mathbf{Z}}_{b}$ and the original
source features, across bootstrap replicates and algorithms. Further, we make
calculate these scores separately for training, development, and testing splits.
The training and development splits are subsets of $I$. Training samples were
used in the optimization for each feature learner; development samples were used
to choose hyperparameters of this optimization. They are shown separate from the
test samples $I^{C}$ to make it possible to recognize potential overfitting in
the feature learning process.

Like Figure \ref{fig:distributed_hm}, this figure provides further evidence that
the feature learning process recovers important aspects of the source features.
It further demonstrates that, even after the initial dimensionality reduction,
only several CCA directions have high canonical correlations. The dimensionality
reduction method used does not have a strong influence. In general, the fraction
of data belonging to $I$ also matters little; however, there is an exception in
the second dimension of the CNN features (top row). Here, the features learned
when $\absarg{I}$ is only 15\% of the data have noticeably lower canonical
correlation relative to when more data are used for feature learning.

The feature learning algorithms do not seem to overfit the simulation data. If
they did, then the canonical correlations on the training and development data
would be larger than those on the test data. However, there are noticeable
differences between the data splits, and this pattern will recur in later
figures. We hypothesize that the differences are due to the alignment process.
During the Procrustes rotation, one matrix $Z^{b}\left[I^{C}\right]$ may get
``lucky'' and learn an alignment with axes that better reflect variation in the
source features. In this way, even though the test data may not have been used
to learn features, they may have higher correlation to the source than the
original training features.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{cca_summary}
  \caption{The top canonical correlations between the sources features and the
    aligned $\protect\underaccent{\bar}{\mathbf{Z}}_{b}$. Horizontal rows correspond to
    CNN, RCF, and VAE algorithms. Columns give the CCA dimension. Within each
    display, the canonical correlations across all bootstraps is shown. The
    $x$-axis within each cell gives the amount of data used for feature learning
    and the dimensionality reduction method used. Canonical correlations are
    computed separately for samples within training, development, and test
    splits, indicated by each point's color. The total amount of data used for
    feature learning does not strongly influence the canonical correlation
    between source and learned features. From the point of view of these
    canonical correlations, the RCF and VAE have comparable feature learning
    behaviors.}
  \label{fig:cca_summary}
\end{figure}

To shed further light on the effect of using different sized $\absarg{I}$
training samples, Figure \ref{fig:selection_summary2} shows the median number of
features selected by stability selection $\absarg{S\left(\lambda,
  \pi_{thr}\right)}$ across training sample sizes. The median is taken across
all bootstrap iterations $b = 1, \dots, 20$. We fix a regularization strength
$\lambda$ where small perturbations in $\pi_{thr}$ can lead to large changes in
the number of selected features. We have run stability selection for $250$
replicates, all restricted to either training, development, or test data, as
indicated by the color of each line.

All the curves decrease because increasing the stringency $\pi_{thr}$ leads to
fewer selected features. We find that the features learned by the CNN are more
frequently selected. This is expected, considering that the CNN features are
learned in a supervised fashion. More surprising is the fact that the RCF
features are more frequently selected than the VAE features, suggesting that the
simple baseline might already provide features useful for interpretation.

Finally, it appears that $\absarg{S\left(\lambda, \pi_{thr}\right)}$ is largest
when using 50\% of the data for feature learning. Though the features learned
using 90\% of the data may be higher quality, it is not possible to select them
more frequently, because stability selection will have low power when it can
only subsample from the 10\% of the data reserved for inference. This phenomenon
appears for features learned using both supervised and unsupervised methods. For
this reason, in the remainder of this study, we will focus on results obtained
using a 50\% split between learning and inference -- that is, $absarg{I} =
\absarg{I^{C}}$ -- though results using different splits are available in the
appendix.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{selection_summary2}
  \caption{The median number of features selected by stability selection across
    bootstrap runs $\protect\underaccent{\bar}{\mathbf{Z}}_{b}$, viewed as a function of
    $\pi_{thr}$. Rows and columns correspond to models and fraction of the data
    used for training, respectively. Stability selection is run within training,
    validation, and test splits (color) and using two dimensionality reduction
    procedures during alignment (solid vs. dashed lines).}
  \label{fig:selection_summary}
\end{figure}

\subsection{Stability study}

Figures \ref{fig:embeddings_sim} and \ref{fig:selections_sim} summarize the
results of Algorithms \ref{fig:fss} and \ref{alg:selection} applied to several
feature learners. For this figure, $\absarg{I} = \absarg{I^{C}} = 5000$ samples.
The analogous results when using a different fraction of the data for training
are given in the appendix.

Within a single panel, each star glyph corresponds to a single sample. The
arms of the glyph link the bootstrap replicates for that given sample:
$\protect z_{i}^{1}, \dots, z_{i}^{20}$. Each glyph is colored in the true
value of $y_{i}$ for that sample. Only the first two learned feature
dimensions are shown. Both the model and reduction strategy used influence
the stability of the learned features and their association with the response

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{embeddings50-test}
  \caption{Example aligned features
    $\protect\underaccent{\bar}{\mathbf{Z}}_{b}$ derived from several models
    and alignment strategies applied to the simulation data with 50\% of the
    data reserved for feature learning. Displays on the top and bottom were
    obtained using SVD and SCA-reduced features, respectively. The three columns
    correspond to features learned from the CNN, RCF, and VAE. For clarity, only
    a subset of samples is shown, and all belong to $I^{C}$. The analogous
    figure with both $I$ and $I^{C}$ is given in the appendix.}
  \label{fig:embeddings50-test}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{selection_paths50}
  \caption{Stability selection paths $\hat{\Pi}_{k}^{b}\left(\lambda\right)$ for
    10 learned features, across models and alignment strategies. (a - c) and (d
    - f) use SVD and SCA-reduced features, respectively. (a) and (d) are derived
    from CNN features, (b) and (e) from RCF features, and (c) and (f) from VAE
    features. Within each group, a single panel corresponds to one learned
    feature dimension. Each curve gives $\hat{\Pi}_{k}^{b}\left(\lambda\right)$
    for one bootstrap replicate $b$ and restricted to samples from either the
    training, development, or test set, indicated by the curve color. The
    $x$-axis is the regularization strength $\log \lambda$ and the $y$-axis is
    the probability of selecting that feature at the given regularization
    strength. 50\% of the simulation data was used for training each feature
    learner.}
  \label{fig:selection_paths50}
\end{figure}

Reading the embeddings figures
* Two axes are two dimensions of embeddings. We've chosen those that have high
selection scores
* Each star is a single sample, linked across bootstrap runs (there are 20 arms)
* Color is the true value of y
* Different panels show the different splits. First two are $I$, last is
$I^{C}$.

Interpretation about the embeddings -- difference in algorithms and
dimensionality reduction approaches?
* Different samples have different embedding stabilities. Seems like points near
the center are more stable, which makes sense -- the clouds of points along each
subspace are harder to align near the boundaries
* The clusters seem tigheter for SCA. More easily embeddable.
* CNN (and RCF?) features are more obviously associated with the response
* Top features in VAE are not necessarily associated with the response. Note
that this is in spite of the fact that the reconstructions are often quite good
(see appendix figure)

Interpretation about the selections -- difference in the algorithms and
dimensionality reduction approaches
* There is no obvious overfitting as far as feature learning is concerned.
* Selection paths $\Pi$ are not monotone for SCA. Likely due to the fact that
those scores can be correlated with one another in this approach, but not for
SVD.
