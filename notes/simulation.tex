
\section{Simulation}

We use a simulation experiment to understand properties of the proposed
algorithms. Our key questions are,

\begin{enumerate}
\item When we vary the relative sizes of $I$ and $I^{C}$, we expect a trade-off
  between feature learning and inferential quality. What is the nature of that
  trade-off?
\item How different are the features learned by supervised and unsupervised
  feature learning algorithms $T\left(\cdot; \theta\right)$?
\item Does the choice of dimensionality reduction used in $\mathcal{A}$ affect
  downstream inferences?
\item Is it ever possible to substitute the RCF feature learner for either of
  the more time-consuming VAE or CNN models?
\end{enumerate}

To answer these questions, we evaluate our proposal using a full factorial with
three factors,

\begin{enumerate}
\item Relative sizes of $I$ and $I^{C}$: We use $\absarg{I} \in \left\{0.15n, 0.5n, 0.8n\right\}$.
\item Dimensionality reduction procedure: We use both PCA and sparse PCA.
\item Algorithm used: We train CNN, VAE and RCF models.
\end{enumerate}

\subsection{Simulated Data}

The key ingredient in this simulation is the construction of a dataset where all
``true'' features are directly controlled. To motivate the simulation, imagine
that we are examining a set of tumor pathology slides, with the hope of
recovering features that are predictive of survival time. Each pathology slide
gives a view into a cellular ecosystem — there are several types of cells, with
different sizes, density, and affinities to one another.

In the simulation, we first generate these underlying properties of each slide.
Survival is defined as a linear function of these underlying properties. Then,
images are generated that reflect these properties. The essential characteristic
of this simulation is that the analyst only has access to the images and
survival times, not the true predictors of the linear model. A good feature
extractor $T\left(x; \hat{\theta}\left(\mathcal{D}\right)\right)$ should recover
these important cell ecosystem properties from the images alone. Example images
for varying values of $y$ are given in Figure \ref{fig:matern_example}.

We now give details. Our images are simulated as two-dimensional marked Log Cox
Matern Process (LCMP) \cite{diggle2013}. Recall that a Matern process is a
Gaussian process with covariance function

\begin{align*}
C_{\nu, \alpha}(\|x - y\|)=\sigma^{2} \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\sqrt{2 \nu} \frac{\|x - y\|}{\alpha}\right)^{\nu} K_{\nu}\left(\sqrt{2 \nu} \frac{\|x - y\|}{\alpha}\right).
\end{align*}

$\alpha$ acts like a bandwidth parameter and $\nu$ controls the roughness of the
simulated process. A LCMP process with $K$ classes is constructed as follows.
First, a nonnegative process $\Lambda\left(x\right)$ is simulated along the
image grid, $\Lambda\left(x\right) \exp{\mathcal{N}\left(0,
  \mathbf{C}_{\nu_{\Lambda}, \alpha_{\Lambda}}\right)}$, where
$\mathbf{C}_{\nu_{\Lambda}, \alpha_{\Lambda}}$ is the covariance matrix induced
by $C_{\nu_{\Lambda}, \alpha_{\Lambda}}$. This represents a baseline intensity
that determines the location of cells, regardless of cell type. Then, $K$
further processes are simulated, $B_{k}\left(x\right) \exp{b_{k} +
  \mathcal{N}\left(0, \mathbf{C}_{\nu_{B}, \alpha_{B}}\right)} $. These
processes will reflect the relative frequencies of the $k$ classes at any
location $x$; the intercept $b_k$ makes a class either more or less frequent
across all positions $x$.

Given these intensity functions, we can simulate $N$ cell locations by drawing
from an inhomogeneous Poisson process with intensity $\Lambda\left(x\right)$.
For a cell at location $x$, we assign it cell type $k$ with probability
$\frac{B_{k}^{\tau}\left(x\right)}{\sum_{k^\prime = 1}^{K}
  B^{\tau}_{k^\prime}\left(x\right)}$. Here, we have introduced a temperature
parameter $\tau$ which controls the degree of mixedness between cell types at a
given location.

To complete the procedure for simulating images, we add two last source of
variation — cell size. The cells from class $k$ are drawn with a random radius
drawn from $\text{Gamma}\left(5, \lambda_{k}\right)$. A summary of all
parameters used to generate each image is given in Table \ref{tab:sim_params}.
These parameters are the ``true'' underlying features associated with the
observed images; they give the most concise description of the variation
observed across the images.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/generation_mechanism}
  \caption{Example simulated images, for low (top), average (middle), and high
    (bottom) values of $y$. For each sample, three relative intensity functions
    are generated, shown as a greyscale heatmap (left three displays for each
    group). Samples drawn from each process are overlaid as circles. The final
    images (display at the right for each group) overlay the points from the
    three processes, discarding the original generating intensity function.
    Evidently, small $y$ values are associated with smoother, less structured
    intensity functions.}
  \label{fig:matern_example}
\end{figure}

\begin{table}[]
\begin{tabular}{|p{0.1\linewidth}|p{0.4\linewidth}|p{0.16\linewidth}|p{0.15\linewidth}|}
\hline
\textbf{Feature}              & \textbf{Description}                                 & \textbf{Influence}          & \textbf{Range}             \\
\hline
$N_i$                & The total number of cells.                                    & 0.5                         & $\left[50, 1000\right]$    \\
\hline
$\nu_{i,\Lambda}$    & The roughness of the underlying intensity process.            & -0.5                        & $\left[0, 8\right]$        \\
\hline
$\alpha_{i,\Lambda}$ & The roughness of the underlying intensity process.            & -0.5                        & $\left[0, 8\right]$        \\
\hline
$b_{ik}$             & The intercept controlling the overall frequency of class $k$. & 1 for $k = 1$,\ -1 otherwise & $\left[-0.15, 0.15\right]$ \\
\hline
$\nu_{iB}$           & The roughness of the relative intensity processes.            & -0.5                        & $\left[0, 3\right]$        \\
\hline
$\alpha_{iB}$        & The bandwidth of relative intensity processes.                & -0.5                        & $\left[0, 3\right]$        \\
\hline
$\tau_{i}$           & The temperature used in cell type assignment.                 & 0.5                         & $\left[0, 3\right]$        \\
\hline
$\lambda_{ik}$       & The shape parameter controlling the sizes of each cell type.  & 1 for $k = 1$,\ 0 otherwise  & $\left[100, 500\right]$   \\
\hline
\end{tabular}
\caption{Our simulation mechanism is governed by the above parameters.
  Parameters $N_i$ and $\lambda_{ik}$ control the number and sizes of imaginary
  cells. $\nu_{i, \Lambda}$, $\alpha_{i, \Lambda}$, $b_{ik}$, $\nu_{iB}$,
  $\alpha_{iB}$, and $\tau_{i}$ control the overall and relative intensities of
  the Matern process from which the cell positions are drawn. Example draws are
  displayed in Figure \ref{fig:matern_example}}
\label{tab:sim_params}
\end{table}


These features are the underlying properties used to generate survival time.
Specifically, we generate a survival time using $y_i = \sum_{k}
\text{Influence}\text{Feature}_{ik}$ for the values Influence given in Table
\ref{tab:sim_params}. Note that there is no additional noise: if the
$\text{Feature}_{ik}$'s were known for each sample, then the $y$'s could be
predicted perfectly. Therefore, the simulation gauges the capacity of the
feature learners to recover these known underlying features. These features are
themselves all drawn from uniform distributions between the ranges given in the
table.

\subsection{Results}

* Learned features are indeed distributed. But they are also highly correlated,
especially for the CNN.
  - average correlation diagram and cca figure
  - The other summary diagram?
* SCA features are easily alignable than PCA features
* The VAE features are not often selected. The feature learning processes are
not interchangeable.

figure 1

interpretation

figure 2

interpretation

figure 3

interpretation
