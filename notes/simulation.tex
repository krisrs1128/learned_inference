
\section{Simulation}

We use a simulation experiment to understand properties of the proposed
algorithms. Our key questions are,

\begin{enumerate}
\item When we vary the relative sizes of $I$ and $I^{C}$, we expect a trade-off
  between feature learning and inferential quality. What is the nature of that
  trade-off?
\item How different are the features learned by supervised and unsupervised
  feature learning algorithms $T\left(\cdot; \theta\right)$?
\item Does the choice of dimensionality reduction used in $\mathcal{A}$ affect
  downstream inferences?
\item Is it ever possible to substitute the RCF feature learner for either of
  the more time-consuming VAE or CNN models?
\end{enumerate}

To answer these questions, we evaluate our proposal using a full factorial with
three factors,

\begin{enumerate}
\item Relative sizes of $I$ and $I^{C}$: We use $\absarg{I} \in \left\{0.15n, 0.5n, 0.8n\right\}$.
\item Dimensionality reduction procedure: We use both PCA and sparse PCA.
\item Algorithm used: We train CNN, VAE and RCF models.
\end{enumerate}

\subsection{Simulated Data}

The key ingredient in this simulation is the construction of a dataset where all
``true'' features are directly controlled. To motivate the simulation, imagine
that we are examining a set of tumor pathology slides, with the hope of
recovering features that are predictive of survival time. Each pathology slide
gives a view into a cellular ecosystem — there are several types of cells, with
different sizes, density, and affinities to one another.

In the simulation, we first generate these underlying properties of each slide.
Survival is defined as a linear function of these underlying properties. Then,
images are generated that reflect these properties. The essential characteristic
of this simulation is that the analyst only has access to the images and
survival times, not the true predictors of the linear model. A good feature
extractor $T\left(x; \hat{\theta}\left(\mathcal{D}\right)\right)$ should recover
these important cell ecosystem properties from the images alone. Example images
for varying values of $y$ are given in Figure \ref{fig:matern_example}.

We now give details. Our images are simulated as two-dimensional marked Log Cox
Matern Process (LCMP) \cite{diggle2013}. Recall that a Matern process is a
Gaussian process with covariance function

\begin{align*}
C_{\nu, \alpha}(\|x - y\|)=\sigma^{2} \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\sqrt{2 \nu} \frac{\|x - y\|}{\alpha}\right)^{\nu} K_{\nu}\left(\sqrt{2 \nu} \frac{\|x - y\|}{\alpha}\right).
\end{align*}

$\alpha$ acts like a bandwidth parameter and $\nu$ controls the roughness of the
simulated process. A LCMP process with $K$ classes is constructed as follows.
First, a nonnegative process $\Lambda\left(x\right)$ is simulated along the
image grid, $\Lambda\left(x\right) \exp{\mathcal{N}\left(0,
  \mathbf{C}_{\nu_{\Lambda}, \alpha_{\Lambda}}\right)}$, where
$\mathbf{C}_{\nu_{\Lambda}, \alpha_{\Lambda}}$ is the covariance matrix induced
by $C_{\nu_{\Lambda}, \alpha_{\Lambda}}$. This represents a baseline intensity
that determines the location of cells, regardless of cell type. Then, $K$
further processes are simulated, $B_{k}\left(x\right) \exp{b_{k} +
  \mathcal{N}\left(0, \mathbf{C}_{\nu_{B}, \alpha_{B}}\right)} $. These
processes will reflect the relative frequencies of the $k$ classes at any
location $x$; the intercept $b_k$ makes a class either more or less frequent
across all positions $x$.

Given these intensity functions, we can simulate $N$ cell locations by drawing
from an inhomogeneous Poisson process with intensity $\Lambda\left(x\right)$.
For a cell at location $x$, we assign it cell type $k$ with probability
$\frac{B_{k}^{\tau}\left(x\right)}{\sum_{k^\prime = 1}^{K}
  B^{\tau}_{k^\prime}\left(x\right)}$. Here, we have introduced a temperature
parameter $\tau$ which controls the degree of mixedness between cell types at a
given location.

To complete the procedure for simulating images, we add two last source of
variation — cell size. The cells from class $k$ are drawn with a random radius
drawn from $\text{Gamma}\left(5, \lambda_{k}\right)$. A summary of all
parameters used to generate each image is given in Table \ref{tab:sim_params}.
These parameters are the ``true'' underlying features associated with the
observed images; they give the most concise description of the variation
observed across the images.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{generation_mechanism}
  \caption{Example simulated images, for low (top), average (middle), and high
    (bottom) values of $y$. For each sample, three relative intensity functions
    are generated, shown as a greyscale heatmap (left three displays for each
    group). Samples drawn from each process are overlaid as circles. The final
    images (display at the right for each group) overlay the points from the
    three processes, discarding the original generating intensity function.
    Evidently, small $y$ values are associated with smoother, less structured
    intensity functions.}
  \label{fig:matern_example}
\end{figure}

\begin{table}[]
\begin{tabular}{|p{0.1\linewidth}|p{0.4\linewidth}|p{0.16\linewidth}|p{0.15\linewidth}|}
\hline
\textbf{Feature}              & \textbf{Description}                                 & \textbf{Influence}          & \textbf{Range}             \\
\hline
$N_i$                & The total number of cells.                                    & 0.5                         & $\left[50, 1000\right]$    \\
\hline
$\nu_{i,\Lambda}$    & The roughness of the underlying intensity process.            & -0.5                        & $\left[0, 8\right]$        \\
\hline
$\alpha_{i,\Lambda}$ & The roughness of the underlying intensity process.            & -0.5                        & $\left[0, 8\right]$        \\
\hline
$b_{ik}$             & The intercept controlling the overall frequency of class $k$. & 1 for $k = 1$,\ -1 otherwise & $\left[-0.15, 0.15\right]$ \\
\hline
$\nu_{iB}$           & The roughness of the relative intensity processes.            & -0.5                        & $\left[0, 3\right]$        \\
\hline
$\alpha_{iB}$        & The bandwidth of relative intensity processes.                & -0.5                        & $\left[0, 3\right]$        \\
\hline
$\tau_{i}$           & The temperature used in cell type assignment.                 & 0.5                         & $\left[0, 3\right]$        \\
\hline
$\lambda_{ik}$       & The shape parameter controlling the sizes of each cell type.  & 1 for $k = 1$,\ 0 otherwise  & $\left[100, 500\right]$   \\
\hline
\end{tabular}
\caption{Our simulation mechanism is governed by the above parameters.
  Parameters $N_i$ and $\lambda_{ik}$ control the number and sizes of imaginary
  cells. $\nu_{i, \Lambda}$, $\alpha_{i, \Lambda}$, $b_{ik}$, $\nu_{iB}$,
  $\alpha_{iB}$, and $\tau_{i}$ control the overall and relative intensities of
  the Matern process from which the cell positions are drawn. Example draws are
  displayed in Figure \ref{fig:matern_example}}
\label{tab:sim_params}
\end{table}


These features are the underlying properties used to generate survival time.
Specifically, we generate a survival time using $y_i = \sum_{k}
\text{Influence}\text{Feature}_{ik}$ for the values Influence given in Table
\ref{tab:sim_params}. Note that there is no additional noise: if the
$\text{Feature}_{ik}$'s were known for each sample, then the $y$'s could be
predicted perfectly. Therefore, the simulation gauges the capacity of the
feature learners to recover these known underlying features. These features are
themselves all drawn from uniform distributions between the ranges given in the
table.

\subsection{Results}

Figure \ref{fig:distributed_hm} shows the activations of learned features across
2000 samples for two bootstrap resamples from each of the feature learning
algorithms. In the three cases, the learned features correspond to

\begin{itemize}
\item CNN: Activations from the last layer of neurons, used directly as input
  for the regression. There are a total of 512 nonnegative
  features\footnote{They are nonnegative because they follow an ReLU layer.}.
\item VAE: Spatially-pooled activations from the middle, encoding layer of the
  variational autoencoder. There are a total of 64 real-valued features.
\item RCF: The spatially-pooled activations corresponding to each of 1048 random
  convolutional features.
\end{itemize}

Our first observation is that there is no simple correspondence between learned
and source features (i.e., parameters of the underlying simulation). For
example, it is not the case that one set of features represents the number of
cells $N$, and a different set maps to the roughness $\alpha, \nu$. Rather,
there appear to be clusters of learned features, and each cluster corresponds to
a pattern of correlation across multiple source features. We also find large
subsets of features, across all models, that are only weakly correlated with any
source feature.

Next, note that certain source features are ``easier'' to represent than others,
in the sense that more of the learned features are strongly correlated with
them. Many features are correlated with $N_{i}$, the total number of cells in
the image, and $\lambda_{i1}$, the size of the cells from Process 1. Depending
on the model the process roughness $\alpha_{ir}$, $\nu_{ir}$ and prevalence
$\beta_{ik}$ parameters are either only weakly or not at all correlated with the
learned features. Interestingly, the convolutional network learns to represent
$\beta_{1}$ well, but not $\beta_{2}$ or $\beta_{3}$ -- this is consistent with
the fact that only $\beta_{1}$ influences the response $y_{i}$.

Finally, we observe that different models learn qualitatively different sets of
features. For example, the CNN features tend to be more clustered, with strong
correlation across several source features. In contrast, the RCF features show
more gradual shifts in correlation strength. They also show relatively little
variation in correlation strength across features other than $\lambda_{i1}$ and
$N_{i}$.

Note that the features do not naturally map onto one another from one run to the
next. This is not obvious from Figure \ref{fig:distributed_hm}, but a zoomed in
version in Figure \ref{fig:distributed_hm_subset} showing only the first 15
features per run makes this clear.

It is natural to ask whether the overall correlation between source and learned
features are different between the different algorithms. To this end, we compute
the top canonical correlations between these sets of features, shown in Figure
\ref{fig:cca_sim}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{correlation_heatmap}
  \caption{Each feature learning algorithm (rows) learns a distributed
    representation of the true underlying features in the simulation. The two
    columns give results from two bootstrap resamples. Within each heatmap, rows
    correspond to the parameters in Table \ref{tab:sim_params}. Columns are
    activations of learned features; they have been reordered by a hierarchical
    clustering. The color of a cell gives the correlation between the associated
    true and learned features. Green and burgundy encode positive and negative
    correlation, respectively. Evidently, all learned features are associated
    with multiple true features at once.}
  \label{fig:distributed_hm}
\end{figure}

* Learned features are indeed distributed. But they are also highly correlated,
especially for the CNN.
  - average correlation diagram and cca figure
  - The other summary diagram?
* SCA features are easily alignable than PCA features
* The VAE features are not often selected. The feature learning processes are
not interchangeable.

figure 1

interpretation

figure 2

interpretation

figure 3

interpretation
