\documentclass[11pt]{article}
\usepackage{graphicx,amsmath,amssymb}
\usepackage{outlines}
\usepackage{enumitem}
\setenumerate[1]{label=\Roman*.}
\setenumerate[2]{label=\Alph*.}
\setenumerate[3]{label=\roman*.}
\setenumerate[4]{label=\alph*.}


\title{}
\author{Kris Sankaran}
\begin{document}
\maketitle

\begin{outline}
\1 Two concrete examples
  \2 Collaborator thinks that mixing between cell types might be related to
  survival in cancer
  \2 Epidemiologist things that certain kinds of built environment / pollution
  are associated with disease
  \2 Traditionally: derive features manually
    \3 Define tumor invasion scores
    \3 Label polluted sites
  \2 Limits of traditional / the current frontier
    \3 Manually extracting features is a time-consuming process
    \3 Even if you had the patience to extract features, there is always the
    potential that you are missing some features that are relevant
\1 Associated abstraction
  \2 Problem definition
    \3 We want to perform formal inference on features / encodings that were
    algorithmically derived, where the encoder is treated as a black box
      \4 $X \in \mathbb{R}^{n \times p}$ are original features. Likely
      not meaningful, like the raw pixel values of an image, or vectorized
      adjacency matrix of a graph.
      \4 $z\left(\cdot; \hat{\eta}\left(X\right)\right) : \mathbb{R}^{p} \to
      \mathbb{R}^{k}$ is a learned encoder, with parameters $\eta$. Possible to
      also learn encoder in a supervised way: $\eta\left(X, y\right)$.
      \4 Denote $z^{\ast} = z\left(x^{\ast}; \hat{\eta}\left(X\right)\right)$ or
      $z_{i} = z\left(x_{i}; \hat{\eta}\left(X\right)\right)$ are encodings when
      you apply to new (or old) data $x^\ast$ or $x_{i}$, respectively.
      \4 We want something like a variable importance / stability score for the
      $k^{th}$ coordinate of $z$, with respect to the outcome $y$ accounting for
      the fact that $z$ was learned in a more complex way
    \3 We want (something like?) FDR control guarantees: If we collected a new
    dataset, and derived a new set of features from the blackbox, then the new
    set of significant learned features should be very related to those learned
    originally
      \4 FDR control isn't the correct notion, however, because we can't simply
      call a feature a false positive or false negative feature.
      \4 To the extent that there is any signal relating $X$ to $y$, then each
      of the coordinates of $z$ is likely to exhibit some relationship. As in
      the PCA analogy, even when all the coordinates are in principle truly
      related to the response, some of the coordinates are going to be targeted
      for interpretation (people come up with a story to interpret different PCA
      axes after the fact). We want to provide information that helps temper or
      strengthen those downstream interpretations.
    \3 We want to evaluate the stability of learned features from sample to
    population
  \2 Analogy: performing hypothesis tests on SPLS / PCA features derived from a
  large table.
    \3 The main difference now is that we'll work with features are derived from
    more complex data, which are not amenable to linear reductions: images and
    graphs.
\1 Related literature
  \2 What have people done, and how we're different
    \3 We are linked to literature on interpretability of black box models
      \4 Both they and us want to leverage newer, less structured in scientific
      applications
      \4 More than equipping people with new tools for interacting with
      encodings, we want to provide mechanisms for inference
      \4 It's important for us that we provide quantitative guidance about the
      reproducibility of a (blackbox analysis)
      \4 Example work: look up references in Raghu and Schmidt section 7
    \3 We are linked to literature on computational inference in
    high-dimensional models
      \4 Our encoders will map raw data into high-dimensional spaces, and we
      will need to screen lots of candidate features in order to find a few that
      are really worth following-up on, scientifically
      \4 Unlike that work, however, we're not going to assume that the features
      are given to us
      \4 Example work: stability selection, knockoffs, post-selection inference,
      split sample inference
\1 Contributions of this paper
  \2 Warning vs. reassurance: the features we learn from run to another are
  similar / different
    \3 in case different, here are some ways you can consolidate them
    \3 Quantification of scale of randomness driven by data vs. algorithm
  \2 Familiarity vs. surprise: features used after trying to optimize some
  metric aren't / are okay to use in inferential procedures
  \2 Natural computational speedups, and their validity in this setting
  (positive or negative result)
  \2 Conceptual overview of an increasingly common task in statistical
  analysis, along with implementation for others to follow along
\end{outline}

\section{Background}

\begin{outline}
\1 Intro
  \2 We draw on ideas from both computational (high-dimensional?) inference and
  deep feature learning
    \3 We need methods from deep feature learning to come up with encodings for
    graphs and images
    \3 We need inferential methods to provide statistical guarantees for the
    things we're learning from deep learning models
  \2 Also leverage methods from multiview data analysis, since we're going to
  want to measure the similarity of encoding functions across multiple runs
\1 Stability
  \2 Can start with the Mosteller \& Tukey quote, about when you run an
  experiment again, you should expect a different result
  \2 Main algorithm we'll make use of is stability selection
    \3 summary: method is a way of assigning significance to individual features
    in a high-dimensional linear model
    \3 $\lfloor \frac{n}{2} \rfloor$-sized subsamples are generated. For each, a
    lasso is run across a grid of $\lambda$'s, and the coefficient paths are
    obtained. Then, for each $\lambda$ and each coefficient, the fraction
    $\Pi_{j}\left(\lambda\right)$ of subsamples that had nonzero $\beta_{j}$ for
    that value of $\lambda$ are counted. You can plot these $\Pi_{j}$'s, to get
    a sense of which coefficients are important.
    \3 These $\Pi_{j}$'s satisfy certain FDR control properties. (tk, what were
    they again? I think they're FDR control rates if you select all the
    variables using that particular value of $\lambda$)
  \2 references: Second Course in Regression, Yu's Bernoulli paper, Stability Selection
\1 Feature learning
  \2 Image autoencoders. Will encode on subtiles, and summarize using average
  position of a random selection of subtiles.
    \3 Autoencoder learns a lower-dimensional, nonlinear manifold with the
    highest density, within the very high-dimensional (but mostly empty) image
    space
    \3 The objective function: $\|x - f\left(g\left(x +
    u\right)\right)\|_{2}^{2}$, where $g : \mathbb{R}^{D} \to \mathbb{R}^{K}$
    and $f: \mathbb{R}^{K} \to \mathbb{R}^{D}$ are convolutional networks and
    $u$ is uniform or gaussian noise (with some appropriate scale parameter).
    \3 $f$ and $g$ are usually only a few layers deep, and the whole system is
    trained using (adaptive forms of) gradient descent.
    \3 The added noise prevents the model from learning a simple identify
    function, and allows the dimension of the latent space $K$ to be larger than
    $D$ (in theory, though we won't be doing this).
    \3 Convolution is a restriction over all possible linear layers. It creates
    an ``inductive bias'' which is natural in image contexts, since you want to
    be able to recognize particular features within an imagine, no matter where
    they are situated.
    \3 Analogy with PCA: If you replace the second term with $VV^{T}x$ and
    optimize over orthogonal $V$, then this is just PCA.
    \3 references: Vincent, Bengio, Dauphin's Denoising Autoencoders.
    Representation learning survey.
  \2 CNNs
    \3 Limits of autoencoders: Like PCA, DAEs are going to try to capture main
    sources of variation, whether or not they are particularly associated with a
    response of particular interest. (Can give usual PCA variation picture in a
    supplement). This makes them suboptimal in situations where we have focused
    attention on particular variables, like survival time or disease rate, as in
    our motivating examples.
    \3 Objective function: $\|y -
    \sigma\left(f\left(x\right)^{T}\beta\right)\|_{2}^{2}$, where $f$ transforms
    the raw image to the ``last layer's'' feature maps, and $\beta$ is the
    regression coefficient for the last layer. $f$ is usually a composition of
    learnable linear and nonlinear transformations.
    \3 In the case that these supervised labels are available, convolutional
    networks can be used to extract features from $x$ that are highly correlated
    with these responses. A lot of deep learning, CNNs to RNNs, can be thought
    of as building nonlinear feature extractors that represent the original $x$
    (images, sequences, ...) in a new feature $z$ that is easily amenable to
    linear methods in the final layer (logistic regression or svm).
    \3 references: representation learning survey, deep learning book
  \2 Graph autoencoders
    \3 The above two methods have proven widely useful across domains with
    image-like data. However, many of the raw data we're interested in are
    graphs, and it turns out that similar approaches can also be used for graph
    data.
    \3 Main conceptual leap is to replace usual image convolutions with graph
    convolutions. From there, graph autoencoders and convolutional networks are
    natural extensions.
    \3 tk write down the objective function, for a particular approach.
    \3 references: Graph deep learning survey. Diffusion wavelets.
  \2 Representation space analysis
    \3 We can use the encodings to navigate types of variation in image and
    graph data
    \3 A common heuristic is that ``deeper'' encodings represent higher-level
    variation, across components
    \3 It's particularly useful to make these encodings interactive, using
    brushing, because the data (encodings) are high-dimensional. Each point is
    associated with a high-dimensional object, not a one-dimensional attribute.
    \3 tk give some reference; maybe Viegas team on tensorflow

\1 Multiview
  \2 Why is it necessary?
    \3 The encoders we learn from one sample to another will exhibit variation
      \4 In fact, even on the same sample, we may learn different encoders, just
      from randomness in the initialization
    \3 Multiview methods give us a way of quantifying the variation across
    learned functions
  \2 Transition, these are the methods that we find useful
  \2 RV coefficient
    \3 A measure of the similarity between two tables
    \3 Write the formula, (tk this is a random guess)
    \begin{align}
      RV\left(X, Y\right) &= \frac{tr\left(X^T Y\right)}{\|X\|_{F}\|Y\|_{F}}
    \end{align}
    \3 geometric interpretation: it's the angle between tables, viewed in an
    inner product space induced by the trace (?)
    \3 reference: Holmes and Josse
  \2 Multi-CCA (via penalized matric decomposition)
    \3 RV coefficient gives us a measure of similarity between encodings on a
    pairwise level, but what can we say when there are many runs to compare?
    \3 Penalized matrix decomposition optimizes (tk?)
    \begin{align}
      \arg\max_{u_1, \dots, u_{k}} \sum_{\text{pairs } u_{k}, u_{k^{\prime}}} \|X_{k}u_{k} - X_{k^{\prime}}u_{k^{\prime}}\|_{2}^{2} + \sum_{k = 1}^{K} \|u_{k}\|_{1} + \|u_{k}\|_{2}
    \end{align}
    for the set of top scores, and then regresses out $u_{k}$'s from each table
    and repeats the process to get the next set of scores.
    \3 Interpretation: If the optimization objective is very low, then there is
    a combination of features for each table that are all measuring the same
    thing. It is a measure of (linear redundancy across all the tables.
    \3 reference: Witten PMD
\end{outline}

\section{Methodology}

\begin{outline}
\1 Establish notation (tk: update the notation in intro to reflect this)
  \2 Raw training data: $x_{1}, \dots, x_{n}$. Each $x_{i} \in \mathbb{R}^{D}$.
  The full sample will be denoted by $\mathbf{X}_{mathcal{D}} \in \mathbb{R}^{n
    \times D}$.
    \3 $D$ can be the dimension of the unwrapped full image (2000 x 2000)
    \3 $D$ can also be the dimension of the unwrapped full adjacency matrix
  \2 Training response: $y_{1}, \dots, y_{n}$. Each $y_{i} \in \mathbb{R}$. Can
  be directly extended to categorical $y_{i}$. The full training response will
  be denoted $\mathbf{y}_{\mathcal{D}} \in \mathbb{R}^{n}$.
  \2 A new sample of raw features and response will be written $\left(x,
  y\right)$.
  \3 Encoders
    \4 All encoders are mappings of the form,
      \begin{align}
        T\left(x; \theta\right): \mathbb{R}^D \to \mathbb{R}^{K},
      \end{align}
      where $K$ is the dimension of the embedding space and $\theta$
      parameterizes the mapping.
    \4 Unsupervised encoders are learned without reference to $\mathbf{y}_{D}$.
    Formally, this means that the parameters of the mapping are learned using
    only $\mathbf{X}_{D}$, which we denote by
    \begin{align}
      \hat{\theta}\left(\mathbf{X}_{\mathcal{D}}\right)
    \end{align},
    in other words, the learned encoder has the form,
    \begin{align}
      T\left(x; \hat{\theta}\left(\mathbf{X}_{\mathcal{D}}\right)\right)
    \end{align}
    \4 Conversely, supervised encoders are allowed to refer to
    $\mathbf{y}_{\mathcal{D}}$ when learning the encoding function,
    \begin{align}
      \hat{\theta}\left(\mathbf{X}_{\mathcal{D}}, \mathbf{y}_{\mathcal{D}}\right).
    \end{align}
    \4 In either case, we will often use the shorthand, $z_{i} = T\left(x_{i},
    \hat{\theta}\right)$.
    \4 Traditionally, the extractor $T$ would have been constructed by hand. It
    would be the max - min of a small time series (as in the ALS challenge
    solution), or the amount of mixing between immune cells (as in cancer
    example). We want to make use of new technology for learning these features.
    \4 Subsampling. It will often be impossible to encode the full object using
    a single convolutional pass. Instead, we will randomly sample subtiles and
    then encode these. Formally, encoder is a composition of patch extractors
    feature extractors. The final encodings result from averaging the encodings on
    individual subtiles,
    \begin{align}
      T\left(x; \theta\right) &= \frac{1}{R}\sum_{r = 1}^{R} G\left(P_{\epsilon_{r}}\left(x\right); \theta\right).
    \end{align}
\1 stability selection of learned features (abstraction)
  \2 Transition and overview. There are extensive literatures on learning good
  mappings $T\left(x, \hat{\theta}\right)$ for all sorts of raw data $x$. Our
  interest is in characterizing the stability of inferences that we draw from
  the $T$'s.
    \3 We'll begin by articulating an approach to drawing inference on learned
    features that ignores the randomness induced by learning $\hat{\theta}$.
    \3 With that subroutine defined, we'll propose a way to wrap it that allows
    us to use stability selection to draw inference that accounts for randomness
    across learning procedures
  \2 Working with a single feature extractor
    \3 This are procedures for testing the significance of features learned
    using a single encoding model.
    \3 Supervised case: Referring to response
      \4 Split the data. Defined disjoint union $\mathcal{D} = \mathcal{D}_{1} \cup \mathcal{D}_{2}$.
      \4 Learn features using one half.
      $\hat{\theta}\left(\mathbf{D}_{\mathcal{D}_{1}}\right)$. The optimization
      routine will vary from architecture to architecture.
      \4 Now evaluate $T\left(x^{\ast};
      \hat{\theta}\left(\mathbf{X}_{\mathcal{D}_{1}}\right)\right)$ for each
      $x^{\ast} \in \mathcal{D}_{2}$. Stack the results in to an encodings
      matrix $\mathbf{Z}_{2} \in \mathbb{R}^{\left|\mathcal{D}_{1}\right| \times
        K}$.
      \4 Perform stability selection on $\mathbf{Z}_{2}, \mathbf{y}_{2}$, where
      $y_{2}$ are the coordinates of $\mathbf{y}$ in $\mathcal{D}_{2}$. That is,
      compute lasso trajectories for many subsamples and see which columns of
      $\mathbf{Z}_{2}$ are most heavily correlated with $\mathbf{y}_{2}$.
      \4 We'll examine in the simulation section what happens when you don't
      split $\mathcal{D} = \mathcal{D}_{1} \cup \mathcal{D}_{2}$. (do the FDR
      control properties of stability selection still hold?)
    \3 Unuspervised case: Not referring to response
      \4 Use full data. We can just learn $T\left(\cdot;
      \hat{\theta}\left(\mathbf{X}_{\mathcal{D}}\right)\right)$
      \4 Analogously, have $\mathbf{Z} \in \mathbb{R}^{\left|\mathcal{D}\right| \times K}$.
      \4 Then, we perform stability selection on $\mathbf{y}, \mathbf{Z}$.
  \2 Consolidating multiple feature extractors
    \3 Limitation of previous approach: No sense of how reproducible the feature
    learner $T$ will be on a new training sample $\mathbf{X}^{\ast}$.
    \3 Approach to consolidation. We will suppose that the features are learned
    with reference to the response. This will be helpful for us in the
    simulations, and it is the most general setup. However, we don't expect it
    to be as common as the unsupervised case in practice.
      \4 Split the data into two parts, $\mathcal{D} = \mathcal{D}_{1} \cup \mathcal{D}_{2}$.
      \4 Create bootstrap samples by resampling from $\mathcal{D}_{1}$. That is,
      let $\hat{F}_{\mathcal{D}_{1}}$ be the empirical distribution associated
      with samples in $\mathcal{D}_{1}$, and draw,
      \begin{align}
        x_{i}^{b} \overset{i.i.d.}{\sim} \hat{F}_{\mathcal{D}_{1}}
      \end{align}
      for $i = 1, \dots, \left|\mathcal{D}_{1}\right|$ and $b = 1, \dots, B$.
      Call the resulting datasets $\mathcal{D}_{1,1}^{\ast}, \dots,
      \mathcal{D}_{1, B}^{\ast}$.
      \4 Learn encoders using each of these datasets,
      \begin{align}
        \hat{\theta}^{\ast}\left(\mathbf{X}_{\mathcal{D}_{1,1}^{\ast}}\right), \dots, \hat{\theta}^{\ast}\left(\mathbf{X}_{\mathcal{D}_{1,B}^{\ast}}\right)
      \end{align},
      which induce bootstrapped encodings,
      \begin{align}
        \mathbf{Z}_{1,1}^{\ast}, \dots, \mathbf{Z}_{1,B}^{\ast}
      \end{align}
      \4 Perform multiCCA on the induced encodings. This results in a first
      consolidated feature, across all the tables,
      \begin{align}
        \mathbf{Z}_{1,1}^{\ast}v_{11}, \dots, \mathbf{Z}_{1, B}^{\ast}v_{1,B}
      \end{align}
      which we'll just denote as $u_{11}, \dots, u_{1,B}$. You can compute
      subsequent dimensions using usual multiCCA, yielding $u_{j,1}, \dots,
      u_{j,B}$ for any $j$, potentially (but usually not) up to $K$.
      \4 If you ran a lasso of $u_{\cdot b}$ onto $\mathbf{y}_{1}$, you would
      get one set of coefficient paths. You could do this for each $b = 1,
      \dots, B$, and since the dimensions of multiCCA are all designed to be
      correlated with one another, we end up with ``coefficient bands'' instead
      of paths.
      \4 We can rank the features using these bands. Just like in usual
      stability selection, we find the fraction $\Pi_{j}\left(\lambda\right)$ of
      times that the $j^{th}$ dimension from the multiCCA is nonzero at that
      $\lambda$. Only difference is our bootstraps are across entire encoding
      sets, not just across subsamples of $X$.
      \4 You can also visualize trajectory bands, and scatterplot clouds. Just
      like old CCA work where there were pairs of points for each sample -- now
      we have $B$ point clouds per sample.
\1 Algorithm Table
  \2 Inputs:
  \2 Outputs:
  \2 Step 1:
  \2 Step 2:
\1 Scalable approximations
  \2 Challenge: learning each feature extractor can be very time consuming, if
  you're starting from scratch each time. We'll try to accelerate the bootstrap
  computations by using an initial learned encodings to provide appropriate
  hints to subsequent samples.
  \2 Approach using coresets
    \3 All datasets contain redundant samples. We can potentially accelerate the
    feature learning process but initially subsampling to a smaller set which
    maintains the diversity of the original one / discards the redundant samples
    of the initial one.
    \3 To do this, we can leverage proposals from the coresets literature. The
    main parameters are the coreset approximation quality and the coreset sample
    size. Neither of these choices is obvious at the outset.
    \3 In theory, we could do this coreset calculation in the raw feature space.
    However, we expect the encoding space to be much more informative, so we
    will learn an encoder once, before doing any bootstrapping.
      \4 This results in $\tilde{\mathcal{D}}_{1}$, which will be much smaller
      than $\mathcal{D}_{1}$.
      \4 This is the data from from which the bootstrap samples are drawn.
  \2 Approach using fine-tuning
    \3 An alternative is to work with the full data, but use fewer steps in
    training, by initializing intelligently.
    \3 We will learn an encoder using all the data once. This yields
    $\hat{\theta}_{0}$.
    \3 All bootstrap models are then (at least partially) initialized using
    $\hat{\theta}_{0}$.
    \3 The main tuning choice here is how much of the original parameter to copy
    over into each of the bootstrap samples.
  \2 Question for simulation: What is the computation / inference trade-off?
    \3 The fine-tuning approach in particular will potentially suffer from
    lower diversity in the types of encodings that are learned. It may not
    reflect the type of variation we would actually expect when we retrain an
    encoder on a new dataset.
\1 Visualization
  \2 Single encoding
    \3 Linked brushing to see the raw tiles associated with each type of encoding
  \2 Consolidated features
    \3 Project onto the multiview feature space, and look at point clouds. Each
    cloud is from one sample, across all the bootstrap samples.
\end{outline}

\section{Simulations}

\begin{outline}
  \1 Approaches with a single feature extractor
    \2 Questions
      \3 How bad is the type I error rate when you don't first split the data,
      before learning the encoding?
        \4 If very bad, then this is an important warning
        \4 If not bad, this is surprising behavior, and is worth follow-up
        investigation, because it could allow us to use more data
      \3 To what extent do we recover the known sources of variation, and is the
      recovery much better in supervised approaches?
        \4 To what extent are the correlations ``spread out'' across many
        different embedding dimensions? To what extent are separate sources
        concentrated in a few dimensions?
    \2 Data generation mechanism
      \3 We generate data that look like cells, but where particular
      characteristics of the cell ecosystem are associated with some
      survival-like response
      \3 Number of cells is poisson, locations are matern, and sizes are gamma
      distributed
        \4 Approach 1: parameters of these underlying distributions are used to
        simulate survival times
        \4 Approach 2: new statistics (density, mixing across different scales,
        moments of cell sizes) are computed from these images, which are then
        used to simulate survival times
        \4 Approach 1 is nice because everything that's varying across the
        images is directly captured by the generative model parameters. Approach
        2 is potentially easier to interpret, though, and it helps reinforce the
        analogy that feature learning algorithms are just automatic ways of
        deriving features that we might have constructed by hand
    \2 Experimental summaries (to answer questions)
    \2 Figure summarizing results
      \3 Axes labels
      \3 Marks?
    \2 Takeaways from figure
  \1 Power analysis (single learner)
    \2 Describe goal for this simulation
    \2 Somewhat complex, since features are not defined in advance
    \2 Approach: How are features correlated with known factors of variation
    \2 Data generation
    \2 Which methods are applied, with what parameters
    \2 Figure summarizing results
      \3 Axes labels
      \3 Marks?
      \3 panels?
    \2 Takeaways from figure
  \1 Stability across learners
    \2 Describe goal for this simulation
    \2 Simulation setup
      \3 Explain similarities and differences from previous approach
      \3 Validity
      \3 Power
  \1 Computational approximations
    \2 Describe goal for this simulation
      \3 Full, coresets, and fine-tuning, which approach is fastest while being
      faithful to the variation we expect across encoders?
    \2 How much less diverse are the learned encodings?
    \2 Simulation setup
    \2 Figure explaining results
      \3 Plot of time vs. multiCCA agreement across samples
    \2 Takeaways from figure
  \1 Algorithmic vs. data-driven randomness
    \2 Describe goal for this simulation
    \2 Simulation setup
    \2 Figure explaining results
    \2 Takeaways from figure
\end{outline}

\section{Data analyses}

\begin{outline}
  \1 Introduction
    \2 Want to illustrate these ideas on a real problem
      \3 Link it back ot the illustrative example in the introduction
    \2 Description of the data
      \3 MIBI-TOF, or Imaging Cytof data
  \1 Goal of the study
    \2 How are spatial features related to survival in breast cancer?
    \2 Notion of tumor invasion and heterogeneity
\end{outline}

\section{Discussion and conclusions}

\begin{outline}
  \1 Main takeaways from this work
    \2 Warning vs. reassurance: the features we learn from run to another are
    similar / different
      \3 in case different, here are some ways you can consolidate them
      \3 Quantification of scale of randomness driven by data vs. algorithm
    \2 Familiarity vs. surprise: features used after trying to optimize some
    metric aren't / are okay to use in inferential procedures
    \2 Natural computational speedups, and their validity in this setting
    (positive or negative result)
    \2 Conceptual overview of an increasingly common task in statistical
    analysis, along with implementation for others to follow along
  \1 Future work
    \2 We usually have more than raw features. Interaction between raw and
    original features?
      \3 Example: For each cell, we may have a measure of the amounts of
      different proteins or transcripts there
    \2 How can you detect whether the bootstrapped feature extractors would be
    similar or not, without running so much computation?
    \2 Nonlinear procrustes
      \3 Multi-CCA is nice, but what happens if linear correlation is too
      restricted?
    \2 How to deal with features learned at multiple scales? We can imagine
    sampling new cells within the same person, new people within the same
    population, ...
\end{outline}

\bibliographystyle{plain}
\bibliography{refs.bib}
\end{document}
