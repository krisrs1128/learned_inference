\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{graphicx,amsmath,amssymb}
\usepackage{outlines, natbib}
\usepackage{palatino}
\usepackage{enumitem, url}
\setenumerate[1]{label=\Roman*.}
\setenumerate[2]{label=\Alph*.}
\setenumerate[3]{label=\roman*.}
\setenumerate[4]{label=\alph*.}

\title{Measuring Stability in Learned Features}
\author{Kris Sankaran}
\begin{document}
\maketitle

\begin{abstract}
Many modern datasets don't fit neatly into $n \times p$ matrices, but most
techniques for measuring statistical stability expect rectangular data. This
talk describes ways to adapt techniques from statistical stability to the regime
where features must be learned. We are specifically motivated by recent
practices in earth observation science and spatial genomics, where (1) it is
beneficial to use automated feature extractors and (2) these learned features
are subject to scientific scrutiny.

We propose approaches using stability selection and the RV coefficient, applied
to the full feature learning workflow. We design controlled simulations to
characterize the power and practicality of competing ideas. A key challenge is
the amount of computation required by stability selection. To this end, we
propose shortcuts, using fine-tuning and coresets, and gather experimental data
that sheds light on the computation-inference tradeoffs at play.

Finally, we describe case studies from earth observation and genomics data
analyses, where the tools of machine learning can augment the scientist's
workflow, but where guarantees of statistical reproducibility are still central.
\end{abstract}

\begin{outline}
\1 Two concrete examples
  \2 Collaborator thinks that mixing between cell types might be related to
  survival in cancer
  \2 Epidemiologist things that certain kinds of built environment / pollution
  are associated with disease
  \2 Traditionally: derive features manually
    \3 Define tumor invasion scores
    \3 Label polluted sites
  \2 Limits of traditional / the current frontier
    \3 Manually extracting features is a time-consuming process
    \3 Even if you had the patience to extract features, there is always the
    potential that you are missing some features that are relevant
\1 Associated abstraction
  \2 Analogy: performing hypothesis tests on SPLS / PCA features derived from a
  large table \citep{rao1955estimation, elguero1988confidence}.
    \3 The main difference now is that we'll work with features are derived from
    more complex data, which are not amenable to linear reductions: images and
    graphs.
  \2 Problem definition
    \3 We want to perform formal inference on features / encodings that were
    algorithmically derived, where the encoder is treated as a black box
      \4 $\mathbf{X}_ \in \mathbb{R}^{n \times D}$ are original features. Likely
      not meaningful, like the raw pixel values of an image, or vectorized
      adjacency matrix of a graph.
      \4 $T\left(\cdot; \hat{\theta}\left(\mathbf{X}\right)\right) :
      \mathbb{R}^{D} \to \mathbb{R}^{K}$ is a learned encoder, with parameters
      $\theta$. Possible to also learn encoder in a supervised way:
      $\theta\left(\mathbf{X}, \mathbf{y}\right)$.
      \4 Denote $z^{\ast} = T\left(x^{\ast};
      \hat{\theta}\left(\mathbf{X}\right)\right)$ or $z_{i} = T\left(x_{i};
      \hat{\theta}\left(\mathbf{X}\right)\right)$ are encodings when you apply
      to new (or old) data $x^\ast$ or $x_{i}$, respectively.
      \4 We want an importance + stability score for the $k^{th}$ coordinate of
      $z$, with respect to the outcome $\mathbf{y}$ accounting for the fact that
      $z$ was learned in a more complex way
    \3 We want something like FDR control guarantees: If we collected a new
    dataset, and derived a new set of features from the blackbox, then the new
    set of significant learned features should be very related to those learned
    originally
      \4 FDR control isn't the correct notion, however, because we can't simply
      call a feature a false positive or false negative feature.
      \4 To the extent that there is any signal relating $\mathbf{X}$ to
      $\mathbf{y}$, then each of the coordinates of $z$ is likely to exhibit
      some relationship. As in the PCA analogy, even when all the coordinates
      are in principle truly related to the response, some of the coordinates
      are going to be targeted for interpretation (people come up with a story
      to interpret different PCA axes after the fact). We want to provide
      information that helps temper or strengthen those downstream
      interpretations.
    \3 We want to evaluate the stability of learned features from sample to
    population
\1 Related literature
  \2 What have people done, and how we're different
    \3 We are linked to literature on interpretability of black box models
      \4 Both they and us want to leverage newer, less structured in scientific
      applications
      \4 More than equipping people with new tools for interacting with
      encodings, we want to provide mechanisms for inference
      \4 It's important for us that we provide quantitative guidance about the
      reproducibility of a (blackbox analysis)
      \4 Example work: look up references in \citep{raghu2020survey} section 7
    \3 We are linked to literature on computational inference in
    high-dimensional models
      \4 Our encoders will map raw data into high-dimensional spaces, and we
      will need to screen lots of candidate features in order to find a few that
      are really worth following-up on, scientifically
      \4 Unlike that work, however, we're not going to assume that the features
      are given to us
      \4 Example work: stability selection, knockoffs, post-selection inference,
      split sample inference
\1 Contributions of this paper
  \2 Warning vs. reassurance: the features we learn from run to another are
  similar / different
    \3 in case different, here are some ways you can consolidate them
    \3 Quantification of scale of randomness driven by data vs. algorithm
  \2 Familiarity vs. surprise: features used after trying to optimize some
  metric aren't / are okay to use in inferential procedures
  \2 Natural computational speedups, and their validity in this setting
  (positive or negative result)
  \2 Conceptual overview of an increasingly common task in statistical
  analysis, along with implementation for others to follow along
\end{outline}

\section{Background}

\begin{outline}
\1 Intro
  \2 We draw on ideas from both computational (high-dimensional?) inference and
  deep feature learning
    \3 We need methods from deep feature learning to come up with encodings for
    graphs and images
    \3 We need inferential methods to provide statistical guarantees for the
    things we're learning from deep learning models
  \2 Also leverage methods from multiview data analysis, since we're going to
  want to measure the similarity of encoding functions across multiple runs
\1 Stability
  \2 Can start with the Mosteller \& Tukey quote, about when you run an
  experiment again, you should expect a different result
  \2 Main algorithm we'll make use of is stability selection
    \3 summary: method is a way of assigning significance to individual features
    in a high-dimensional linear model
    \3 $\lfloor \frac{n}{2} \rfloor$-sized subsamples are generated. For each, a
    lasso is run across a grid of $\lambda$'s, and the coefficient paths are
    obtained. Then, for each $\lambda$ and each coefficient, the fraction
    $\Pi_{j}\left(\lambda\right)$ of subsamples that had nonzero $\beta_{j}$ for
    that value of $\lambda$ are counted. You can plot these $\Pi_{j}$'s, to get
    a sense of which coefficients are important.
    \3 These $\Pi_{j}$'s satisfy certain FDR control properties. (tk, what were
    they again? I think they're FDR control rates if you select all the
    variables using that particular value of $\lambda$)
  \2 references: \citep{mosteller1977data, yu2013stability, meinshausen2010stability}
\1 Feature learning
  \2 VAE's: We encode images into $K$ (usually 64) dimensional vectors. This
  gives a vectorial representation of the original image.
    \3 Autoencoder learns a lower-dimensional, nonlinear manifold with the
    highest density, within the very high-dimensional (but mostly empty) image
    space
    \3 The objective function: $\|x - f\left(g\left(x +
    u\right)\right)\|_{2}^{2}$, where $g : \mathbb{R}^{D} \to \mathbb{R}^{K}$
    and $f: \mathbb{R}^{K} \to \mathbb{R}^{D}$ are convolutional networks and
    $u$ is uniform or gaussian noise (with some appropriate scale parameter).
    \3 $f$ and $g$ are usually only a few layers deep, and the whole system is
    trained using (adaptive forms of) gradient descent.
    \3 Convolution is a restriction over all possible linear layers. It creates
    an ``inductive bias'' which is natural in image contexts, since you want to
    be able to recognize particular features within an imagine, no matter where
    they are situated.
    \3 Analogy with PCA: If you replace the second term with $VV^{T}x$ and
    optimize over orthogonal $V$, then this is just PCA.
    \3 references: Stochastic Backpropagation, AE Variational Bayes,
    Representation learning survey.
  \2 CNNs
    \3 Limits of autoencoders: Like PCA, DAEs are going to try to capture main
    sources of variation, whether or not they are particularly associated with a
    response of particular interest. (Can give usual PCA variation picture in a
    supplement). This makes them suboptimal in situations where we have focused
    attention on particular variables, like survival time or disease rate, as in
    our motivating examples.
    \3 Objective function: $\|y -
    \sigma\left(f\left(x\right)^{T}\beta\right)\|_{2}^{2}$, where $f$ transforms
    the raw image to the ``last layer's'' feature maps, and $\beta$ is the
    regression coefficient for the last layer. $f$ is usually a composition of
    learnable linear and nonlinear transformations.
    \3 In the case that these supervised labels are available, convolutional
    networks can be used to extract features from $x$ that are highly correlated
    with these responses. A lot of deep learning, CNNs to RNNs, can be thought
    of as building nonlinear feature extractors that represent the original $x$
    (images, sequences, ...) in a new feature $z$ that is easily amenable to
    linear methods in the final layer (logistic regression or svm).
    \3 references: representation learning survey, deep learning book
  \2 Graph autoencoders
    \3 The above two methods have proven widely useful across domains with
    image-like data. However, many of the raw data we're interested in are
    graphs, and it turns out that similar approaches can also be used for graph
    data.
    \3 Main conceptual leap is to replace usual image convolutions with graph
    convolutions. From there, graph autoencoders and convolutional networks are
    natural extensions.
    \3 tk write down the objective function, for a particular approach.
    \3 references: Graph deep learning survey. Diffusion wavelets.
  \2 Representation space analysis
    \3 We can use the encodings to navigate types of variation in image and
    graph data
    \3 A common heuristic is that ``deeper'' encodings represent higher-level
    variation, across components
    \3 It's particularly useful to make these encodings interactive, using
    brushing, because the data (encodings) are high-dimensional. Each point is
    associated with a high-dimensional object, not a one-dimensional attribute.
    \3 tk give some reference; maybe Viegas team on tensorflow

\1 Multiview
  \2 Why is it necessary?
    \3 The encoders we learn from one sample to another will exhibit variation
      \4 In fact, even on the same sample, we may learn different encoders, just
      from randomness in the initialization
    \3 Multiview methods give us a way of quantifying the variation across
    learned functions
  \2 Transition, these are the methods that we find useful
  \2 RV coefficient
    \3 A measure of the similarity between two tables
    \3 Write the formula,
    \begin{align}
      RV\left(X, Y\right) &= \frac{\text{tr}\left(\Sigma_{XY}\Sigma_{YX}\right)}{\|\Sigma_{X}\|_{F}\|\Sigma_{Y}|_{F}}
    \end{align}
    \3 geometric interpretation: it's the angle between tables, viewed in an
    inner product space induced by the trace (?)
    \3 reference: \citep{josse2016measuring}
  \2 Multi-CCA (via penalized matric decomposition)
    \3 RV coefficient gives us a measure of similarity between encodings on a
    pairwise level, but what can we say when there are many runs to compare?
    \3 Penalized matrix decomposition optimizes
    \begin{align}
      \arg\max_{u_1, \dots, u_{k}} \sum_{\text{pairs } u_{k}, u_{k^{\prime}}} \|X_{k}u_{k} - X_{k^{\prime}}u_{k^{\prime}}\|_{2}^{2} + \sum_{k = 1}^{K} \|u_{k}\|_{1} + \|u_{k}\|_{2}
    \end{align}
    for the set of top scores, and then regresses out $u_{k}$'s from each table
    and repeats the process to get the next set of scores.
    \3 Interpretation: If the optimization objective is very low, then there is
    a combination of features for each table that are all measuring the same
    thing. It is a measure of (linear redundancy across all the tables.
    \3 reference: \citep{witten2009penalized}
  \2 Nonlinear redundancy \citep{donnat2018tracking}
    \3 A limitation of the rv-coefficient and multicca approaches is that they
    only consider linear redundancy. This is probably sufficient, since deep
    models should be transforming data to a form that is more or less gaussian
    in the encoding space, but it still may be possible to do better.
    \3 As an alternative, consider the induced graphs created by each
    bootstrapped encoding functions. These graphs can be compared using any sort
    of graph distance.
    \3 If all the induced graphs are similar, we can consider the analysis stable.
\end{outline}

\section{Methodology}

\begin{outline}
\1 Establish notation
  \2 Raw training data: $x_{1}, \dots, x_{n}$. Each $x_{i} \in \mathbb{R}^{D}$.
  The full sample will be denoted by $\mathbf{X}_{mathcal{D}} \in \mathbb{R}^{n
    \times D}$.
    \3 $D$ can be the dimension of the unwrapped full image (2000 x 2000)
    \3 $D$ can also be the dimension of the unwrapped full adjacency matrix
  \2 Training response: $y_{1}, \dots, y_{n}$. Each $y_{i} \in \mathbb{R}$. Can
  be directly extended to categorical $y_{i}$. The full training response will
  be denoted $\mathbf{y}_{\mathcal{D}} \in \mathbb{R}^{n}$.
  \2 A new sample of raw features and response will be written $\left(x,
  y\right)$.
  \3 Encoders
    \4 All encoders are mappings of the form,
      \begin{align}
        T\left(x; \theta\right): \mathbb{R}^D \to \mathbb{R}^{K},
      \end{align}
      where $K$ is the dimension of the embedding space and $\theta$
      parameterizes the mapping.
    \4 Unsupervised encoders are learned without reference to $\mathbf{y}_{D}$.
    Formally, this means that the parameters of the mapping are learned using
    only $\mathbf{X}_{D}$, which we denote by
    \begin{align}
      \hat{\theta}\left(\mathbf{X}_{\mathcal{D}}\right)
    \end{align},
    in other words, the learned encoder has the form,
    \begin{align}
      T\left(x; \hat{\theta}\left(\mathbf{X}_{\mathcal{D}}\right)\right)
    \end{align}
    \4 Conversely, supervised encoders are allowed to refer to
    $\mathbf{y}_{\mathcal{D}}$ when learning the encoding function,
    \begin{align}
      \hat{\theta}\left(\mathbf{X}_{\mathcal{D}}, \mathbf{y}_{\mathcal{D}}\right).
    \end{align}
    \4 In either case, we will often use the shorthand, $z_{i} = T\left(x_{i},
    \hat{\theta}\right)$.
    \4 Traditionally, the extractor $T$ would have been constructed by hand. It
    would be the max - min of a small time series (as in the ALS challenge
    solution), or the amount of mixing between immune cells (as in cancer
    example). We want to make use of new technology for learning these features.
    \4 Subsampling. It will often be impossible to encode the full object using
    a single convolutional pass. Instead, we will randomly sample subtiles and
    then encode these. Formally, encoder is a composition of patch extractors
    feature extractors. The final encodings result from averaging the encodings on
    individual subtiles,
    \begin{align}
      T\left(x; \theta\right) &= \frac{1}{R}\sum_{r = 1}^{R} G\left(P_{\epsilon_{r}}\left(x\right); \theta\right).
    \end{align}
\1 stability selection of learned features (abstraction)
  \2 Transition and overview. There are extensive literatures on learning good
  mappings $T\left(x, \hat{\theta}\right)$ for all sorts of raw data $x$. Our
  interest is in characterizing the stability of inferences that we draw from
  the $T$'s.
    \3 We'll begin by articulating an approach to drawing inference on learned
    features that ignores the randomness induced by learning $\hat{\theta}$.
    \3 With that subroutine defined, we'll propose a way to wrap it that allows
    us to use stability selection to draw inference that accounts for randomness
    across learning procedures
  \2 Working with a single feature extractor
    \3 This are procedures for testing the significance of features learned
    using a single encoding model.
    \3 Supervised case: Referring to response
      \4 Split the data. Defined disjoint union $\mathcal{D} = \mathcal{D}_{1} \cup \mathcal{D}_{2}$.
      \4 Learn features using one half.
      $\hat{\theta}\left(\mathbf{D}_{\mathcal{D}_{1}}\right)$. The optimization
      routine will vary from architecture to architecture.
      \4 Now evaluate $T\left(x^{\ast};
      \hat{\theta}\left(\mathbf{X}_{\mathcal{D}_{1}}\right)\right)$ for each
      $x^{\ast} \in \mathcal{D}_{2}$. Stack the results in to an encodings
      matrix $\mathbf{Z}_{2} \in \mathbb{R}^{\left|\mathcal{D}_{1}\right| \times
        K}$.
      \4 Perform stability selection on $\mathbf{Z}_{2}, \mathbf{y}_{2}$, where
      $y_{2}$ are the coordinates of $\mathbf{y}$ in $\mathcal{D}_{2}$. That is,
      compute lasso trajectories for many subsamples and see which columns of
      $\mathbf{Z}_{2}$ are most heavily correlated with $\mathbf{y}_{2}$.
      \4 We'll examine in the simulation section what happens when you don't
      split $\mathcal{D} = \mathcal{D}_{1} \cup \mathcal{D}_{2}$. (do the FDR
      control properties of stability selection still hold?)
    \3 Unuspervised case: Not referring to response
      \4 Use full data. We can just learn $T\left(\cdot;
      \hat{\theta}\left(\mathbf{X}_{\mathcal{D}}\right)\right)$
      \4 Analogously, have $\mathbf{Z} \in \mathbb{R}^{\left|\mathcal{D}\right| \times K}$.
      \4 Then, we perform stability selection on $\mathbf{y}, \mathbf{Z}$.
  \2 Consolidating multiple feature extractors
    \3 Limitation of previous approach: No sense of how reproducible the feature
    learner $T$ will be on a new training sample $\mathbf{X}^{\ast}$.
    \3 Approach to consolidation. We will suppose that the features are learned
    with reference to the response. This will be helpful for us in the
    simulations, and it is the most general setup. However, we don't expect it
    to be as common as the unsupervised case in practice.
      \4 Split the data into two parts, $\mathcal{D} = \mathcal{D}_{1} \cup \mathcal{D}_{2}$.
      \4 Create bootstrap samples by resampling from $\mathcal{D}_{1}$. That is,
      let $\hat{F}_{\mathcal{D}_{1}}$ be the empirical distribution associated
      with samples in $\mathcal{D}_{1}$, and draw,
      \begin{align}
        x_{i}^{b} \overset{i.i.d.}{\sim} \hat{F}_{\mathcal{D}_{1}}
      \end{align}
      for $i = 1, \dots, \left|\mathcal{D}_{1}\right|$ and $b = 1, \dots, B$.
      Call the resulting datasets $\mathcal{D}_{1,1}^{\ast}, \dots,
      \mathcal{D}_{1, B}^{\ast}$.
      \4 Learn encoders using each of these datasets,
      \begin{align}
        \hat{\theta}^{\ast}\left(\mathbf{X}_{\mathcal{D}_{1,1}^{\ast}}\right), \dots, \hat{\theta}^{\ast}\left(\mathbf{X}_{\mathcal{D}_{1,B}^{\ast}}\right),
      \end{align}
      which induce bootstrapped encodings,
      \begin{align}
        \mathbf{Z}_{1,1}^{\ast}, \dots, \mathbf{Z}_{1,B}^{\ast}
      \end{align}
      when you apply the ensemble of encoding functions on the original dataset
      $\mathcal{D}_{1}$. Note that you cannot simply use the encodings on the
      bootstrap samples themselves, because they there is no easy way to align
      the separate datasets (the rows must correspond).
      \4 Perform multiCCA on the induced encodings. This results in a first
      consolidated feature, across all the tables,
      \begin{align}
        \mathbf{Z}_{1,1}^{\ast}v_{11}, \dots, \mathbf{Z}_{1, B}^{\ast}v_{1,B}
      \end{align}
      which we'll just denote as $u_{11}, \dots, u_{1,B}$. You can compute
      subsequent dimensions using usual multiCCA, yielding $u_{j,1}, \dots,
      u_{j,B}$ for any $j$, potentially (but usually not) up to $K$.
      \4 If you ran a lasso of $u_{\cdot b}$ onto $\mathbf{y}_{1}$, you would
      get one set of coefficient paths. You could do this for each $b = 1,
      \dots, B$, and since the dimensions of multiCCA are all designed to be
      correlated with one another, we end up with ``coefficient bands'' instead
      of paths.
      \4 We can rank the features using these bands. Just like in usual
      stability selection, we find the fraction $\Pi_{j}\left(\lambda\right)$ of
      times that the $j^{th}$ dimension from the multiCCA is nonzero at that
      $\lambda$. Only difference is our bootstraps are across entire encoding
      sets, not just across subsamples of $X$.
      \4 You can also visualize trajectory bands, and scatterplot clouds. Just
      like old CCA work where there were pairs of points for each sample -- now
      we have $B$ point clouds per sample.
\1 Algorithm Table
  \2 Inputs:
  \2 Outputs:
  \2 Step 1:
  \2 Step 2:
\1 Scalable approximations
  \2 Challenge: learning each feature extractor can be very time consuming, if
  you're starting from scratch each time. We'll try to accelerate the bootstrap
  computations by using an initial learned encodings to provide appropriate
  hints to subsequent samples.
  \2 Approach using coresets
    \3 All datasets contain redundant samples. We can potentially accelerate the
    feature learning process but initially subsampling to a smaller set which
    maintains the diversity of the original one / discards the redundant samples
    of the initial one.
    \3 To do this, we can leverage proposals from the coresets literature. The
    main parameters are the coreset approximation quality and the coreset sample
    size. Neither of these choices is obvious at the outset.
    \3 In theory, we could do this coreset calculation in the raw feature space.
    However, we expect the encoding space to be much more informative, so we
    will learn an encoder once, before doing any bootstrapping.
      \4 This results in $\tilde{\mathcal{D}}_{1}$, which will be much smaller
      than $\mathcal{D}_{1}$.
      \4 This is the data from from which the bootstrap samples are drawn.
    \3 https://github.com/obachem/kmc2
  \2 Approach using fine-tuning
    \3 An alternative is to work with the full data, but use fewer steps in
    training, by initializing intelligently.
    \3 We will learn an encoder using all the data once. This yields
    $\hat{\theta}_{0}$.
    \3 All bootstrap models are then (at least partially) initialized using
    $\hat{\theta}_{0}$.
    \3 The main tuning choice here is how much of the original parameter to copy
    over into each of the bootstrap samples.
  \2 Question for simulation: What is the computation / inference trade-off?
    \3 The fine-tuning approach in particular will potentially suffer from
    lower diversity in the types of encodings that are learned. It may not
    reflect the type of variation we would actually expect when we retrain an
    encoder on a new dataset.
\1 Visualization
  \2 Single encoding
    \3 Linked brushing to see the raw tiles associated with each type of encoding
  \2 Consolidated features
    \3 Project onto the multiview feature space, and look at point clouds. Each
    cloud is from one sample, across all the bootstrap samples.
    \3 You could create a bootstrap visualization that considers the $n$
    original samples are corners on a polygon. The original sample is at the
    center, and differently weighted samples occur within the polygon, closest
    to the samples that it has large fractions of. This can be used for brushed
    selection.
\end{outline}

\section{Simulations}

\begin{outline}
  \1 Approaches with a single feature extractor
    \2 Questions
      \3 How bad is the type I error rate when you don't first split the data,
      before learning the encoding?
        \4 If very bad, then this is an important warning
        \4 If not bad, this is surprising behavior, and is worth follow-up
        investigation, because it could allow us to use more data
      \3 To what extent do we recover the known sources of variation, and is the
      recovery much better in supervised approaches?
        \4 To what extent are the correlations ``spread out'' across many
        different embedding dimensions? To what extent are separate sources
        concentrated in a few dimensions?
    \2 Data generation mechanism
      \3 We generate data that look like cells, but where particular
      characteristics of the cell ecosystem are associated with some
      survival-like response
      \3 Simulation Parameters
        \4 n = 200 samples. Want to have enough samples to do inference, but not
        so many that training takes forever
        \4 image size = 512 x 512. Normal samples are ~ 2000 x 2000, but this is
        just a simulation
        \4 graph construction = geometric graph with radius = ?, knn with K = ?
      \3 Number of cells is poisson, types / locations are marked matern, and
      sizes are gamma distributed
        \4 Approach 1: parameters of these underlying distributions are used to
        simulate survival times
        \4 Approach 2: new statistics (density, mixing across different scales,
        moments of cell sizes) are computed from these images, which are then
        used to simulate survival times
        \4 Approach 1 is nice because everything that's varying across the
        images is directly captured by the generative model parameters. Approach
        2 is potentially easier to interpret, though, and it helps reinforce the
        analogy that feature learning algorithms are just automatic ways of
        deriving features that we might have constructed by hand
        \4 Reminders: matern process is gaussian process with matern covariance
        function,
        \begin{align}
          C_{\nu}(\|x - y\|)=\sigma^{2} \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\sqrt{2 \nu} \frac{\|x - y\|}{\rho}\right)^{\nu} K_{\nu}\left(\sqrt{2 \nu} \frac{\|x - y\|}{\rho}\right),
        \end{align}
        so $\rho$ acts like a bandwidth parameter, and $\nu$ controls the
        roughness. We additionally have relative intensity functions to control
        how much mixing happens between classes -- this is described in
        \citep{diggle2013spatial}.
    \2 Experimental measurements
      \3 validity question
        \4 Generate survival times completely independently of the simulated
        images
        \4 We first run the supervised and unsupervised, single feature extractor
        protocol, from above, with and without splitting the data
        \4 Look at the stability selection trajectories, and see whether any
        seem significant
      \3 power question
        \4 Now generate survival times from the generated cell image properties,
        with a few degrees of signal vs. noise (for ``easiest'' experiment, the
        survival time is basically deterministic, given image properties).
        \4 For each snr level, look at the stability selection curves, and see
        whether any consistently stand out over the rest
        \4 For each snr level, compute the covariance matrix between known and
        estimated features. Sort estimated features by their stability selection
        proportions at some fixed $\lambda$. Are the average / extreme
        correlations concentrated, or diffuse, among significant variables?
    \2 Figure summarizing results
      \3 Trellis of stability selection curves
        \4 Bottom panels are when there is no sample splitting
        \4 Top panels are when there is splitting
        \4 SNR increases as you go from left panel to right. Leftmost panel
        tells you how features look when there is no correlation with the
        response.
      \3 Trellis of known vs. learned feature covariances
        \4 Columns (increasing SNR) are as before
        \4 Each panel summarizes two covariance matrix. Above the diagonal are
        covariances in the splitting case. Below the diagonal are covariances
        before splitting.
    \2 Takeaways from figure (guesses, for now)
      \3 Some curves will appear significant in the no-splitting, no signal
      situation, due only to overfitting
      \3 No single learned feature captures any given known features. The known
      feature will be reflected in many of the embedding dimensions.
        \4 random idea, maybe should do some sort of ICA after embedding
      \3 As SNR increases, more features are stability-selected, and the
      correlations with known features increases.
  \1 Stability across learners
    \2 Questions
      \3 Do you get very different learned features due to random initialization
      (same dataset)? Do you get very different learned features across
      bootstrap samples (different-ish data)?
      \3 Can we identify features that are learned across all the bootstrap
      samples, after appropriate alignment? Can we quantify this property just
      by using the multiCCA + stability band proposed above?
    \2 Simulation setup
      \3 Same data generation protocol as above. Maybe just save datasets from
      above for this part.
      \3 Experiment
        \4 Fix datasets for a few SNRs, and rerun the feature learning algos
        with different initializations (seeds). Otherwise exactly the same.
        \4 From the same SNRs, bootstrap many datasets, and rerun feature
        learning across bootstraps.
        \4 Align the learned feature sets using the multiCCA approach described
        above. For each of those aligned feature sets $u_{\cdot b}$, compute
        lasso trajectories. Idea is that if a given aligned feature is not
        important, it will always be zero for most of the $\lambda$'s.
      \3 Experiment parameters
        \4 How many bootstrap samples? Will probably be a lot of compute, so
        let's start with $\sim 20$, and see whether people complain later
        \4 How many runs with the same initialization?
        \4 Which of the SNR levels from above? Probably don't want to do more
        than ``none, medium, high''
      \3 Summaries and figures
        \4 (Repeat for both initialization and bootstrap experiments, maybe as
        different rows in the same overall figure)
        \4 Stability selection bands: take the quartiles of the trajectories for
        each feature, and use that to construct a band. Min and max can be
        whiskers extending from the band (or something).
        \4 Whiskers analog of covariance matrices: Each panel is a different
        aligned feature $u_{j}$. Along rows, give known features. Show the
        correlation of the current feature across all the known features, for
        each of the bootstrap samples (the same bootstrap sample can be linked
        by a long curve, like parallel coordinates). Deliberate choose a few
        panels that are more or less ``important,'' according to either
        trajectory or correlation measures.
        \4 Scatterplots: Pick a couple pairs of important / unimportant aligned
        dimensions, according to the trajectories above. Arrange these pairs as
        scatterplot axes in a grid. Create small stars for each sample, where
        the spokes point to different bootstrap samples (projected into the
        aligned space).
      \3 Takeaways
        \4 Do the bands get much narrower / stars get smaller when you fix the
        dataset? That would indicate that randomness across datasets is really
        important, and you learn quite different feature extractors.
        \4 Are some significant features more stable than others? There are two
        senses in which this might be the case -- the lasso trajectories might
        be consistently at the top, or the projection into multiCCA scores might
        be very tight.
        \4 Is the correlation between aligned features and known features much
        worse than the non-aligned features? If so, that motivates more work on
        the alignment piece.
        \4 Can we conjecture / guarantee any control properties based on the
        bands, when there is no signal? Are most of the bands intersecting $y =
        0$?
  \1 Computational approximations
    \2 Questions
      \3 Between full training, coresets, and fine-tuning -- which approach is
      fastest while being faithful to the variation we expect across encoders?
      \3 How much less diverse are the learned encodings, when we use
      fine-tuning from some initial network or work with a coreset subsample?
      \3 Can you tell the approximation features apart from those that are
      learned the ``proper'' full dataset, full training way?
      \3 What kinds of diminishing returns should we expect? Is initializing the
      first layer very beneficial, but the last not so important? Is reducing to
      a coreset 10\% the original size even remotely a good idea? Ideally, would
      have some order-of-magnitude rules of thumb to share with people wanting
      to accelerate the bootstrapping process.
    \2 Simulation setup
      \3 Evaluation of fine-tuning
        \4 Using the same bootstrap samples\footnote{Or, same original sample +
          indices into each bootstrap sample, to save some disk space.}, try
        learning full network / autoencoder once, and then use fine-tuning on
        all the bootstrap samples
        \4 Vary across amounts of fine-tuning and signal strengths
        \4 Specifically: vary the number of layers that are initialized using
        the fully trained model. We'll see how that affects the downstream
        diversity of learned features. Signal strengths varied as before.
        \4 At the end, apply the learned feature extractors to the full data.
      \3 Evaluation of coresets
        \4 First compute a coreset of an initial embedding, and then only
        bootstrap the coreset
        \4 Vary the size of the coreset
    \2 Figures
      \3 MultiCCA objective when aligning bootstrap samples vs. number of
      fine-tuning layers. Simple dotplot. Color in the dots according to how
      long it took for differnetly initialized models to converge.
      \3 Richer version: For a few of the SNR levels, compute the stability
      bands (or aligned feature scatterplots) using the full and fine-tuned
      feature extractors.
      \3 Can make each of those figures also for coresets, where we vary coreset
      size rather than fine-tuning position.
    \2 Takeaways from figures
      \3 If you want faster approximations, should you try fine-tuning, or
      coresets, or neither, or it depends?
      \3 To get x\% of the original diversity in bootstrap samples, you should
      fine-tune to $z$ layers at most or reduce to a coreset of size $n$ at
      minimum
\end{outline}

\begin{figure}
  \includegraphics[width=0.8\textwidth]{figure/simulation_example}
  \caption{Three random images selected from the simulation. Each cell is one
    circle, and there are three classes (colors). The sizes, densities, class
    proportions, and degree of mixing between cells is what determines
    survival.}
\end{figure}

\section{Data analyses}

Definitely -- spatial proteomics data.

\begin{outline}
  \1 Introduction
    \2 Want to illustrate these ideas on a real problem
      \3 Link it back ot the illustrative example in the introduction
    \2 Description of the data
      \3 MIBI-TOF, or Imaging Cytof data
      \3 \url{https://github.com/BIRSBiointegration/Hackathon/tree/master/sc-targeted-proteomics}
  \1 Goal of the study
    \2 How are spatial features related to survival in breast cancer?
    \2 Notion of tumor invasion and heterogeneity
  \1 Apply bootstrap + stability approach
    \2 Visualize the most important looking features from the graph methods
    (graph cnn, graph embedding). How do graphs of the samples vary as you move
    along that feature dimension?
    \2 Compare a feature that is always important, and one that is important in
    only some of the bootstrap samples
    \2 Does this resonate biologically: do we recover something like tumor
    heterogenity, cell size, ... something that can be backed up by published
    wet-lab experiment
\end{outline}

Possibly -- remote sensing for public health data.

\begin{outline}
  \1 Introduction
    \2 Want to make the case that this methodology is valuable across a range of
    applications, not just a quirk of modern genomics.
    \2 Will let us focus on graph methods in previous one, and image methods in
    this.
    \2 Other than that, it's similar (description of data, apply method, relate
    to existing public health literature)
    \2 What data to use? Ask Suzanne.
      \3 Starting points: \url{https://arset.gsfc.nasa.gov/sites/default/files/airquality/Health16/week1_final.pdf}
\end{outline}

\section{Discussion and conclusions}

\begin{outline}
  \1 Main takeaways from this work
    \2 Warning vs. reassurance: the features we learn from run to another are
    similar / different
      \3 in case different, here are some ways you can consolidate them
      \3 Quantification of scale of randomness driven by data vs. algorithm
    \2 Familiarity vs. surprise: features used after trying to optimize some
    metric aren't / are okay to use in inferential procedures. If they are okay,
    some idea of the how concentrated real phenomena are within a few features.
    \2 Proposals for measuring the stability of learned features across
    resampled data: aligning features and plotting bands.
    \2 Natural computational speedups, and their validity in this setting
    (positive or negative result)
    \2 Conceptual overview of an increasingly common task in statistical
    analysis, along with implementation for others to follow along
  \1 Future work
    \2 We usually have more than raw features. Interaction between raw and
    original features?
      \3 Example: For each cell, we may have a measure of the amounts of
      different proteins or transcripts there
    \2 How can you detect whether the bootstrapped feature extractors would be
    similar or not, without running so much computation?
    \2 Nonlinear procrustes
      \3 Multi-CCA is nice, but what happens if linear correlation is too
      restricted?
    \2 How to deal with features learned at multiple scales? We can imagine
    sampling new cells within the same person, new people within the same
    population, ...
\end{outline}

Some equations that might come in handy,

\begin{align}
  \log p_{\theta} \geq  \mathbb{E}_{q_{\varphi}}\left[\log p_{\theta}(x \mid z)\right]-D_{KL}\left(q_{\varphi}(z \mid x) \| p_{\theta}(z)\right).
\end{align}

\begin{align}
&\alpha, \nu \\
&\alpha_{r}, \nu_{r} \\
&\beta_{r} \\
&\lambda_{r} \\
&\tau
\end{align}

\begin{align}
C_{\alpha, \nu}(\|x-y\|)= \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\sqrt{2 \nu} \frac{\|x-y\|}{\alpha}\right)^{\nu} K_{\nu}\left(\sqrt{2 \nu} \frac{\|x-y\|}{\alpha}\right)
\end{align}

\begin{align}
y_{i}=\frac{1}{3}\left(\beta_{i1}-\beta_{i2}-\beta_{i3}\right)+\frac{1}{4}\left(\nu_{ir}+\alpha_{ir}+\nu_i+\alpha_i\right)-\tau_{i}+\frac{1}{100}\left(\lambda_{i1}-50\right)+\epsilon_{i}
\end{align}

\begin{align}
\operatorname{RV}(\mathbf{X}, \mathbf{Y})=\frac{\operatorname{tr}(\hat{\Sigma}_{\mathbf{XY}} \hat{\Sigma}_{\mathbf{YX}} }{\sqrt{\operatorname{tr}\left(\hat{\Sigma}_{\mathbf{X} \mathbf{X}}^{2}\right) \operatorname{tr}\left(\hat{\Sigma}_{\mathbf{Y Y}}^{2}\right)}}
\end{align}

\bibliographystyle{plain}
\bibliography{refs.bib}
\end{document}
