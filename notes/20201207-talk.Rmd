---
title: Visualizing Model Stability
author: Kris Sankaran
date: December 7, 2020
output: 
  distill::distill_article:
    css: "style.css"
---
```{r, echo = FALSE}
knitr::opts_chunk$set(cache = TRUE, fig.path = "dec7fig")
```

## Warm-Up: Stability in Topic Models

I've found myself asking two questions when discussing topic models with
practitioners recently,

* How can we compare many topic models at once?
* Our best models are the least interpretable, what should we do?

### Comparing $K$

We spend so much energy trying to "choose K." Why don’t we instead focus on
trying to "compare K"?

This is very related to Sijia’s observation: Choosing $K$ is irrelevant when our
data are hierarchical anyways.

### $K$-means

$K$-means is the simplest case where we can see these ideas play out. This is
what happens when you cluster different types of data with several values of
$K$.

```{r}
knitr::include_graphics("/Users/kris/Desktop/conceptual/lda_compare/notes/talk_20201113_files/figure-html/unnamed-chunk-4-1.png")
```

Then, we can track the membership of individual points across clusters.

```{r, out.width = "50%"}
knitr::include_graphics("/Users/kris/Desktop/conceptual/lda_compare/notes/talk_20201113_files/figure-html/unnamed-chunk-3-1.png")
knitr::include_graphics("/Users/kris/Desktop/conceptual/lda_compare/notes/talk_20201113_files/figure-html/unnamed-chunk-3-2.png")
```

### Proposal

* Put all the topic models in the same space using a correspondence analysis. 
* Use optimal transport to learn an alignment between topics.
* Make this interactive

```{r, out.width = "50%"}
knitr::include_graphics("/Users/kris/Desktop/conceptual/lda_compare/topics_test/transport-2_3.png")
knitr::include_graphics("/Users/kris/Desktop/conceptual/lda_compare/topics_test/transport-4_8.png")
```

Q: I generated the mixed memberships from $\gamma_{i} \sim
\text{Dir}\left(\alpha \mathbf{1}_{K}\right)$. Can you tell what $\alpha$ I
used?

Notice: This isn't restricted to just comparing $K$. We can also compare
$\alpha$.

```{r, out.width = "50%"}
knitr::include_graphics("/Users/kris/Desktop/conceptual/lda_compare/topics_test/transport-alphas-2_3.png")
knitr::include_graphics("/Users/kris/Desktop/conceptual/lda_compare/topics_test/transport-alphas-5_5.png")
```


## Stability of Learned Features

> Among the seven points from GP [Galeano and Pena, 2019], I would like to
re-emphasize the importance of new sources of information. Indeed, images,
videos and audios are typically cheap devices to record data. GP do not mention
recent progress with autoencoders (Hinton and Salakhutdinov 2006; Vincent et al.
2010): when using such techniques, one would again end up with numeric features
which can then be used for further downstream analysis using techniques from
high-dimensional statistics or statistical machine learning (Bühlmann and van de
Geer 2011; Hastie et al. 2015, cf.).

From Peter Bühlmann's
[rejoinder](ftp://ftp.stat.math.ethz.ch/Research-Reports/Other-Manuscripts/buhlmann/STS797.pdf)
on *Data science, big data and statistics*.

### Personal Applications

I find this a very exciting idea. I encounter more and more problems where
people want to incorporate raw data (images, text, sensor streams) into
scientific workflows, but am never sure what is the best way to go about doing
inference.

```{r}
knitr::include_graphics("dec7fig/glaciers.png")
knitr::include_graphics("dec7fig/cells.png")
```

Two examples of scientific data that do not arrive in tabular form.

### Guiding questions

Can we really use high-dimensional statistics do perform inference on learned
features?

* Can we give clear illustrations that might be useful for practitioners?
* Do differences between hand and machine-curated features require changes in
inferential perspective?

### Nuances

1. Using the same data for feature learning and inference can be risky
	* What is a good trade-off?
2. Learned features are "distributed." A single real-world feature activates a
pattern distributed across many neurons.
3. There is randomness in the feature learning process itself. How should we
account for this?
  * In principle, this is true even for hand-generated features

### Controlled setting

We’ll develop a controlled experiment to better understand the
inference-on-learned-features workflow.

* 50K simulated images (64 x 64 pixels) from a marked Matern process
* Number, relative abundances, sizes, and diversity of circles ("cells")
influence a response ("survival")
  - $\alpha, \nu$: Bandwidth and smoothness of the overall process
  - $\alpha_r, \nu_r$: Bandwidth and smoothness of subprocess $r$
  - $\beta_{r}$: Relative intensity of subprocess $r$.
  - $\lambda_r$: Relative size of cells in subprocess $r$
  - $\tau$: Temperature of overall process (controls mixing)
  
```{r}
paths <- list()
for (i in 1:220) {
  paths[[i]] <- sprintf("/Users/kris/Documents/stability_data/pngs/image-%s.png", stringr::str_pad(i, 4, "left", 0))
}

knitr::include_graphics(unlist(paths))
```

### Experimental setup

Two main factors,

* Algorithm used: Unsupervised (variational autoencoder) or supervised (four
layer convolution-relu-batch norm network).

```{r, out.width = "45%"}
knitr::include_graphics(c("dec7fig/autoencoder_cartoon.png", "dec7fig/cnn_cartoon.png"))
```

* Data split: 15\%, 50\% and 90\% of data for feature learning, the rest
reserved for inference.

```{r, out.width = "40%"}
knitr::include_graphics("dec7fig/proportion_tradeoff.png")
```

To characterize stability, repeat each combination on $B = 5$ bootstrap
resamples of the same training split.

### Types of stability

In the spirit of *[Stability
Expanded](https://hdsr.mitpress.mit.edu/pub/ekrhsui8/release/1?readingCollection=69f01655)*,
we should be precise about what we mean by stability. Who should be stable to
what?

I think of two types,

* Feature subspace stability: The learned feature subspace should be stable
across scientific studies
* Selected feature stability: The subspace directions that we believe are
associated with a response should be stable across scientific studies.

### Feature subspace stability

The learned features do not have a natural correspondence across bootstrap runs.
Feature 1 from the first model has nothing to do with feature 1 from the second.

We expect the learned feature subspaces to be similar, though.

```{r out.width = "50%"}
knitr::include_graphics("dec7fig/subspace_stability.png")
```

Q: What are these subspaces the span of?

This is helped by the fact that the learned features are effectively low
dimensional.

```{r out.width = "40%"}
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/vae90_svd_eigs.png")
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/cnn90_svd_eigs.png")
```


I specifically use the penalized matrix decomposition to find subspaces that are
closely aligned. I'm not satisfied with this approach, but it was easy to
implement quickly.

\begin{align*}
v^{\ast}_{1}, \dots, v^{\ast}_{K} := &\arg\max \sum_{k < k^{\prime}} v_{k}X_{k}^{T}X_{k^{\prime}}v_{k^{\prime}} \\
&\text{ subject to } \|v_{k}\|_{1} \leq \lambda
\end{align*}

### Feature selection stability

Given our aligned features, we can try to use ideas from high-dimensional
statistics. I will use stability selection, but really anything could go here.
It's actually not obvious that high-dimensional ideas are necessary here -- I've
already reduced the tables to the top aligned directions.

### VAE

Here are embeddings when using 90\% of the data for training the VAE model.

```{r}
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/embeddings_vae90.png")
```

Q: If the subspaces were more aligned, how would this plot change?

We can look at the selection paths, but it seems that these features are not
actually predictive of the response. It's also not just that the lasso has a
small sample size -- a lasso fitted to just the embedded test data doesn't seem
to pick up on the response.

```{r}
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/selection_paths_vae90.png")
```

Q: If all the features were irrelevant, what would these paths look like?

```{r, out.width = "49%"}
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/y_vs_y_hat-vae90-0.train.png")
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/y_vs_y_hat-vae90-0.test.png")
```

That said, the learned features *do* seem at least somewhat correlated with the
underlying generative factors. The lasso just isn't able to pick up on this.

```{r}
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/vae90_correlation_diagram.png")
```

We can look at the results for the VAE using only 15\% of the data, but the
results don't actually seem that different,

* Stable embeddings
* That can't be used for prediction

```{r}
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/embeddings_vae15.png")
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/y_vs_y_hat-vae15-0.train.png")
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/y_vs_y_hat-vae15-0.test.png")
```

### CNN

For the convolutional network, the MultiCCA doesn't seem to be able to align the
features.

```{r}
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/embeddings_cnn90.png")
```

At first I thought this was because the activations are so skewed. But this is
the case even after `asinh` transforming and slightly blurring them (to remove
the spike at 0).

```{r, out.width = "60%"}
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/cnn90_activation_histogram.png")
```

Unsurprisingly, the selection curves are also quite inconsistent across
bootstrap runs.

```{r}
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/selection_paths_cnn90.png")
```

Nonetheless, the latent dimensions *are* predictive and pick up some of the more
subtle sources of variation ($\tau$ and $\beta_{2}$).

```{r out.width = "49%"}
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/y_vs_y_hat-cnn90-0.train.png")
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/y_vs_y_hat-cnn90-0.test.png")
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/cnn90_correlation_diagram.png")
```

### Acceleration

Running even 5 bootstrap runs for each method is already probably something most
practitioners would not have the patience to do. Can we propose a workflow that
gives similar results, but without the computational burden?

Q: How long do you think it took to train the 6 * 5 models?

Idea: Use random convolutional features to bypass model training altogether.

```{r}
knitr::include_graphics("/Users/kris/Desktop/conceptual/learned_inference/inference/vignettes/activating-patches.png")
```

### Conclusion

> One hallmark of the statistically conscious investigator is a firm belief
that, however the survey, experiment, or observational program actually turned
out, it could have turned out somewhat differently... Most of us find uncertainty
uncomfortable; the history of data analysis can be read as a succession of
searches for certainty about uncertainty.

In *[Data Analysis and Regression](https://bookshop.org/books/data-analysis-and-regression-a-second-course-in-statistics-classic-version/9780201048544)*
by Tukey and Mosteller (page 25)
