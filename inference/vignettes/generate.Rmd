---
title: "Data Generation Mechanism"
output: pdf_document
---

```{r}
library("MASS")
library("dplyr")
library("ggplot2")
library("inference")
library("raster")
library("readr")
library("reshape2")
library("sf")
library("stars")
library("stringr")
set.seed(123)

theme_set(theme_bw() + theme(panel.grid=element_blank()))
scale_x_continuous <- function(...) {
  ggplot2::scale_x_continuous(..., expand = c(0, 0))
}
scale_y_continuous <- function(...) {
  ggplot2::scale_y_continuous(..., expand = c(0, 0))
}
scale_fill_continuous <- function(...) {
  ggplot2::scale_fill_continuous(..., low="white", high="black")
}
```

First, we'll show how to simulate a single image.

```{r}
x <- expand.grid(seq(0.05, 1, 0.05), seq(0.05, 1, 0.05))
intensity <- matern_process(x)
z <- inhomogeneous_process(1000, intensity)
ggplot() +
  geom_tile(aes(x = Var1, y = Var2, fill = z), intensity) +
  geom_point(mapping = aes(x = X1, y = X2), data.frame(z), col = "red") +
  coord_fixed()

probs <- relative_intensities(x, 4, nu = 1)
mprobs <- melt(probs, id.vars = c("Var1", "Var2"))
marks <- mark_process(z, probs, tau = 1) %>%
  mutate(variable = paste0("X", mark))
ggplot() +
  geom_tile(aes(x = Var1, y = Var2, fill=value), mprobs) +
  geom_point(aes(x = X1, y = X2, col = mark, size=size), marks) +
  facet_wrap(~ variable) +
  coord_fixed()
```

Now, let's think about how we want to link the parameters of image generation to
something like a survival time. This is a specification of all the parameters.

```{r}
x <- expand.grid(seq(0.1, 1, 0.05), seq(0.1, 1, 0.05))
n_original <- 100 # number of points
beta_r <- c(-0.5, 0, 0.5)
nu_r <- 1 # smoothness of the relative intensity functions (higher is smoother)
alpha_r <- 0.5 # bandwidth of the relative processes (higher is smoother)
nu <- 2 # smoothness of the overall process
alpha <- 2 # bandwidth of the overall process
tau <- 1.5 # temperature for the marking (0 -> uniform, inf -> hard assignment)
lambdas <- c(20, 50, 100) # rate parameters in gamma controlling size
```

```{r}
result <- sim_wrapper(x, n_original, nu, alpha, beta_r, nu_r, alpha_r, tau, lambdas)
marks <- result$marks %>%
  mutate(variable = paste0("X", mark))
mprobs <- melt(result$probs, id.vars = c("Var1", "Var2"))
ggplot() +
  geom_tile(aes(x = Var1, y = Var2, fill=value), mprobs) +
  geom_point(aes(x = X1, y = X2, col = mark, size=size), marks) +
  facet_wrap(~ variable) +
  coord_fixed() +
  scale_radius(range = c(.1, 5))
```

# Simulating Images

We've sanity checked what all the images look like, when we vary the simulation
parameters. Let's have a way of relating these parameters to an ``outcome''
associated with each image "look." We'll make these assumptions,

* More mixing between cell types is associated with better outcomes
* Less uniform distribution in cell densities is associated with worse outcomes.
* More of the first cell type is better for outcomes, more of the other two is bad.
* larger of the first cell type is better for outcomes

In particular, we'll make up this equation, for some hypothetical outcome y,

 y =  (beta_r[1] - beta_r[2] - beta_r[3]) / 3 + (nu_r + alpha_r + nu + alpha) / 4 - tau + (lambdas[1] - 50) / 100 + noise

so that the signs are in the intuitive direction and so that no one group of
terms dominates. We'll consider modulating the noise level to gauge the
sensitivity of different approaches.

We'll generate the parameters according to a few different distributions (though
they won't be too wild), depending on whether they are allowed to be negative.

```{r}
n_images <- 10
n_original <- runif(n_images, 100, 1000)
beta_r <- 0.3 * runif(n_images * 3, -1, 1) %>%
  matrix(ncol = 3)
nu_r <- runif(n_images, 0, 2)
alpha_r <- runif(n_images, 0, 2)
nu <- runif(n_images, 0, 5) # overall process is (usually) smoother
alpha <- runif(n_images, 0, 5)
tau <- runif(n_images, 0, 5)
lambdas <- runif(n_images * 3, 100, 500) %>%
  matrix(ncol = 3)

# simulate the response
X <- scale(cbind(n_original, beta_r, nu_r, alpha_r, nu, alpha, tau, lambdas[, 1]))
colnames(X) <- c("N", paste0("beta_", 1:ncol(beta_r)), "nu_r", "alpha_r", "nu", "alpha", "tau", "lambda_1")
y <- X %*% c(1, 1, -1, -1, .5, .5, .5, .5, .5, 1) / sqrt(ncol(X))
y <- y - mean(y)
```
Let's create the directory within which to save our results.

```{r}
tiles_dir <- file.path("tiles")
dir.create(tiles_dir)
```

Now we can simulate all the images.

```{r}
# can convert foreach into a standard for loop if no parallel backend
library("parallel")
library("doParallel")
library("foreach")
cl <- makeCluster(detectCores())
registerDoParallel(cl)

pkgs <- c("inference", "dplyr", "MASS", "stringr", "sf", "raster", "reticulate")
paths <- foreach(j = seq_len(n_images), .combine = c, .packages = pkgs) %dopar% {
  # generate the shapefile representing cells
  result <- sim_wrapper(x, n_original[j], nu[j], alpha[j], beta_r[j, ], nu_r[j],
                        alpha_r[j], tau[j], lambdas[j, ])
  marks <- result$marks %>%
    mutate(variable = paste0("X", mark))
  
  # convert to raster and save as numpy
  path <- sprintf(file.path(tiles_dir, "image-%s.npy"), str_pad(j, 4, "left", 0))
  r <- make_raster(marks, 3)
  np <- reticulate::import("numpy")
  np$save(path, as.array(r))
  return(path)
}
stopCluster(cl)

write_csv(data.frame(path = paths, X = X, y = y), file.path("Xy.csv"))
```
