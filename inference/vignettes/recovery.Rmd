---
title: Recovery ability
---

```{r}
library("ggplot2")
library("readr")
library("dplyr")
library("stringr")
library("png")
library("glmnet")
library("yaml")
library("ica")
```

Load the previously extracted features.

```{r}
setwd(Sys.getenv("ROOT_DIR"))
conf <- yaml.load_file("conf/train.yaml")
z <- read_csv(conf$features_dir) %>%
  select(-X1) %>%
  select(path, y, everything())
z$ix <- str_extract(z$path, "[0-9]+") %>%
  as.integer()

y <- z$y
x <- z %>%
  select(matches("[0-9]+")) %>%
  as.matrix()

z_ica <- icafast(x, 5)$S %>%
                     as_tibble() %>%
                     setNames(gsub("V", "z_ica_", names(.))) %>%
                     mutate(ix = z$ix)

cv_res <- cv.glmnet(as.matrix(x), scale(y))
plot(cv_res)
```

Get paths to all the original images, and build an MDS from the learned features.
```{r}
png_paths <- gsub("npy", "png", z$path)
png_paths <- gsub("train", "pngs", png_paths)

ims <- lapply(png_paths, readPNG)
coords <- cmdscale(dist(x))
plot(coords)
```

Plot images along the MDS coordinates.
```{r}
xgrid <- seq(-4, 4.5, by=0.5)
xgrid <- expand.grid(xgrid, xgrid)

plot_data <- list()
for (i in seq_len(nrow(xgrid))) {
  cur_dist <- proxy::dist(xgrid[i, ], coords)
  if (min(cur_dist) < 0.2) {
    im <- ims[[which.min(cur_dist)]]
  } else {
    im <- NULL
  }

  plot_data[[i]] <- list(
    x = xgrid[i, 1],
    y = xgrid[i, 2],
    im = im
  )
}

plot(xgrid, t="n", asp = 1)
for (i in seq_len(nrow(xgrid))) {
  if (is.null(plot_data[[i]]$im)) next

  rasterImage(
    plot_data[[i]]$im,
    xleft = plot_data[[i]]$x - 0.24,
    ybottom = plot_data[[i]]$y - 0.24,
    xright = plot_data[[i]]$x + 0.24,
    ytop = plot_data[[i]]$y + 0.24,
    interpolate = FALSE
  )
}
```

Next, we'll look into which of the learned features are correlated with known
source generating features.

```{r}
joined_features <- read_csv(conf$xy) %>%
  mutate(ix = row_number()) %>%
  left_join(z, by = "ix") %>%
  left_join(z_ica, by = "ix")

features <- list(
  "x" = joined_features %>%
    select(starts_with("X")),
  "y" = joined_features$y.x,
  "z" = joined_features %>%
    select(matches("^[0-9]+")),
  "z_ica" = joined_features %>%
    select(matches("^z_ica"))
)

R <- cor(cbind(features$x, features$y), cbind(features$z), use = "complete")

mR <- melt(R, varnames = c("x", "z"), value.name =  "corr") %>%
  mutate(z = factor(z, levels = hclust(dist(t(R)))$order - 1))

ggplot(mR) +
  geom_tile(aes(x = x, y = z, fill = corr)) +
  scale_fill_gradient2() +
  theme(
    axis.text.x = element_text(angle = 90),
    legend.position = "bottom"
  )
```

It looks like about 25 features are positive or negatively correlated with $N$
(number of cells), $\nu$ (overall smoothness), and $\alpha_r$ (bandwidth of
relative process) simultaneously. A different set of features seems to reflect
$\lambda$, which are the sizes of individual cells. Some overall takeaways, so
far,

* The generative mechanisms are clearly picked up by the learned features, but
  only in a diffuse way. Every learned feature seems to be somewhat correlated
  with all the known generative mechanisms, we are far from the ICA-style
  independent learned features regime. It seems like there are ~ 3 or 4 main
  types of features learned by the autoencoder, each of which is correlated with
  different sets of the generative features.
* The relative fequency features are less strongly correlated with the learned
  features than the size and smoothness features.
* The features that were deliberately chosen to be from heavier-tailed mixture
  distributions are possibly more challenging to recover.
* The correlation with the response $y$ is weaker than with the generating
  mechanisms $X$.
  
We can look at the postprocessed, ICA features, to see whether they confirm our
intuition about the learned features reflecting just a few patterns.

```{r}
R <- cor(cbind(features$x, y = features$y), cbind(features$z_ica), use = "complete")
levs <- paste0("z_ica_", hclust(dist(t(R)))$order)

mR <- melt(R, varnames = c("x", "z_ica"), value.name =  "corr") %>%
  mutate(z_ica = factor(z_ica, levels = levs))

ggplot(mR) +
  geom_tile(aes(x = x, y = z_ica, fill = corr)) +
  scale_fill_gradient2() +
  theme(
    axis.text.x = element_text(angle = 90),
    legend.position = "bottom"
  )
```

It does seem like the top 2 ICA features are essentially picking up cell density
and relative roughness features. The third feature is more clearly related to
the relative amounts of different cells, though, which is a nice finding.
