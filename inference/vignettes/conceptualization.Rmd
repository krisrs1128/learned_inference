---
title: "Conceptualization Experiment"
output: html_document
params:
  feature_variances: "fixed"
  supervised: FALSE
  N: 100
  B: 500
  K: 50
  D: 500
  S: 10
---

```
for s in T F; do
  for S in 0 10 20 30 40; do
    Rscript -e "rmarkdown::render('conceptualization.Rmd', params = list(supervised = $s, S = $S), output_file = paste0('conceptualization-', $s, '-', $S, '.html'))"
  done;
done;
```

```{r}
set.seed(1234)
library(abind)
library(ggplot2)
library(knitr)
library(purrr)
library(reshape2)
library(tidyr)
map(list.files("../R", full.names = TRUE), ~ source(.))
opts_chunk$set(fig.width = 8, fig.height = 8, dpi = 80)
library(ggplot2)
th <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 16),
    legend.position = "bottom"
  )
theme_set(th)

k_switch <- function(L, y) {
  if (params$supervised) {
    k_<- select_features(L, y)
  } else {
    k_ <- 1:ncol(L)
  }
    
  k_
}
```

```{r}
attach(params)
if (feature_variances == "random") {
  fv <- rgamma(D, 1, 2)
} else {
  fv <- 100
}

terms <- algorithmic_features(N, D, K, feature_variances = fv)
response <- simulate_response(terms$L, S)
attach(terms); attach(response)
```

If we actually knew the underlying features, the lasso is able to recover it
reasonably well.

```{r}
library(glmnet)
fit <- glmnet(L, y)
beta_hat <- coef(fit)
plot(beta, beta_hat[-1, 40], asp = 1)
abline(a = 0, b = 1, col = "red")
```


Prediction is easy! This is not news. But can we say anything about any of the
recovered features? 


```{r}
xi <- svd_projector(Z, K)
fit <- glmnet(xi(Z), y)
y_hat <- predict(fit, xi(Z))
plot(y, y_hat[, ncol(y_hat)], asp = 1)
abline(a = 0, b = 1, col = "red")
```

```{r}
max_beta <- which.max(beta)
max_ix <- which.max(abs(cor(xi(Z), L[, max_beta])))
plot(xi(Z)[, max_ix], L[, max_beta])
```

Are they "close" to the true betas in any sense? For this, we need to try
matching the $\hat{L}$ with the underlying $L$ It seems like most of the
features are related in some way to the true predictor columns.

```{r}
round(cor(L[, which(beta != 0)], xi(Z)), 3)
```

It's not clear whether the coefficients are going to be recovered by either
approach, when using stability selection. That said, I still think the stability
curves are going to look quite different when using stability selection on the
fixed features vs. using the random features.

```{r}
#Z_mean <- apply(abind(Zs, along = 3), c(1, 2), mean)
#mean(map_dbl(Zs, ~ sum(. - Z_mean) ^ 2)) /  mean(map_dbl(Z_hats, ~ sum(.^2)))
#N * D / sum((Lambda %*% t(V)) ^ 2)
```

```{r}
Pi_hats <- list()
```

First, let's look at some stability curves for this procedure when using fixed
features.

(if setting supervised = TRUE, What happens to the stability curves when we
deliberately supervise the response? The idea is to pretend the at the
algorithmic feature extractor only returns the learned features $Z$ that are
most correlated with the response. This is a proxy for the fact that our feature
extraction is being modeled, not actually computationally performed. The
approach below does not do any sample splitting.)

```{r}
terms <- sparse_factor_terms(N, D, K, feature_variances = fv)
k_ <- k_switch(L, y)
Z <- algorithmic_features_(terms$L[, k_], terms$V[, k_])

response <- simulate_response(terms$L, S)
xi <- svd_projector(Z, K)
Pi_hats[["unadjusted"]] <- stability_selection(scale(xi(Z)), y, B, fit$lambda)
```

With sample splitting, but no feature re-learning. That is, there is only a
single train / test split and we only learn one set of $Z$'s.

```{r}
train_ix <- sample(N, N / 2)
terms <- sparse_factor_terms(N, D, K, feature_variances = fv)
k_ <- k_switch(terms$L[train_ix, ], y[train_ix])
print(k_)
Z <- algorithmic_features_(terms$L[-train_ix, k_], terms$V[, k_])

xi <- svd_projector(Z, K)
Pi_hats[["split"]] <- stability_selection(scale(xi(Z)), y[-train_ix], B, fit$lambda)
```

```{r}
terms <- sparse_factor_terms(N, D, K, feature_variances = fv)
L_hats <- vector(B, mode = "list")

for (b in seq_len(B)) {
  # different split each time
  train_ix <- sample(N, N / 2)
  k_ <- k_switch(terms$L[train_ix, ], y[train_ix])
  Zb <- algorithmic_features_(terms$L[-train_ix, k_], terms$V[, k_])

  # different projections of random features
  xi <- svd_projector(Zb, K)
  L_hats[[b]] <- scale(xi(Zb))
}

Pi_hats[["relearned"]] <- aligned_stability_curves(L_hats, y)
```

```{r}
library(stringr)
library(dplyr)
Pi <- map_dfr(Pi_hats, ~ as.data.frame(.$Pi) %>% mutate(dim = as.factor(row_number())), .id = "method") %>%
  pivot_longer(starts_with("V")) %>%
  mutate(lambda = as.integer(str_extract(name, "[0-9]+")))
coef_paths <- map_dfr(Pi_hats, ~ melt(.$coef_paths), .id = "method")
```

```{r}
Pi%>%
  arrange(lambda) %>%
  ggplot() +
  geom_line(aes(lambda, value, col = method, group = dim)) + 
  xlim(0, 65) +
  ylim(0, 1) +
  facet_grid(~ method)
```
```{r, fig.width = 20, fig.height = 4}
ggplot(coef_paths %>% filter(Var1 < 30, Var1 > 1, Var3< 500)) +
  geom_line(aes(Var2, value, group = Var3), alpha = 0.1) +
  facet_grid(method~Var1, scale = "free_y")
```



