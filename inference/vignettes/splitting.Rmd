---
title: "R Notebook"
output: html_notebook
---

```{r}
library(dplyr)
library(ggplot2)
library(purrr)
source("../R/simulation.R")
source("../R/stability.R")
```


```{r}
N <- 500
D <- 100
K <- 2
U <- r_ortho(N, D)
V <- r_ortho(D, D)
Sigma <- c(rep(10, 2), rep(0.01, D - K))
X <- U %*% diag(Sigma) %*% t(V) + rmat(N, D, 0.01)

features <- function(X, K = 2, sigma_e = 0.01) {
  N <- nrow(X)
  D <- nrow(X)
  svx <- svd(X)
  
  function(X) {
    Pi <- random_permutation(K)
    (X %*% svx$v[, 1:K] %*% diag(sqrt(svx$d[1:K])) + rmat(N, K, sigma_e)) %*% Pi
  }
}

supervised_features <- function(X, y, K = 2, sigma_e = 0.01) {
  N <- nrow(X)
  D <- nrow(X)
  svx <- svd(X)
  
  sigma_order <- cor(y, svx$u) %>%
    abs() %>%
    order(decreasing = TRUE)
  V <- svx$v[, sigma_order[1:K]]
  
  function(X) {
    Pi <- random_permutation(K)
    (X %*% V %*% diag(sqrt(svx$d[1:K])) + rmat(N, K, sigma_e)) %*% Pi
  }
}
```

If we use an unsupervised feature extractor, there is no problem.

```{r}
y <- rnorm(N)
f <- features(X)
summary(lm(y ~ f(X)))
```


When we don't split the data, the extracted features incorrectly look related to
the response.

```{r}
y <- rnorm(N)
fy <- supervised_features(X, y)
summary(lm(y ~ fy(X)))
```

If we split, then we can see the difference clearly.

```{r}
y <- rnorm(N)
ix <- 1:(N / 2)
fy <- supervised_features(X[ix, ], y[ix])
summary(lm(y[-ix] ~ fy(X[-ix, ])))
```

What if there were actually a relationship between the underlying features and
the response? We can still detect the difference.

```{r}
beta <- c(1.8, -1.8, rep(0, D - 2))
y <- U %*% diag(sqrt(Sigma)) %*% beta + rnorm(N)
ix <- 1:(N / 2)
fy <- supervised_features(X[ix, ], y[ix]) # the "Z"s
summary(lm(y[-ix] ~ fy(X[-ix, ])))
```

Of course, this is only calling the feature extractor once. We'll need to call
it $B$ times in order to be able to evaluate feature stability. The inference
will be on the consensus Z's.